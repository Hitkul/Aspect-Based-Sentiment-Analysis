{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi channel CNN for sentiment analysis\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import remove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import fasttext\n",
    "import csv\n",
    "import codecs\n",
    "import word2vecReader as godin_embedding\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from gensim.models import KeyedVectors\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "def load_data_from_file(filename):\n",
    "    print(\"loading file = \",filename)\n",
    "    sentences = []\n",
    "    label = []\n",
    "    with codecs.open(filename, \"r\",encoding='utf-8', errors='ignore') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            try:\n",
    "                sentences.append(row[0])\n",
    "                label.append(row[1])\n",
    "            except:\n",
    "                print(row)\n",
    "    return sentences,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file =  dataset/final_train.csv\n",
      "loading file =  dataset/final_dev.csv\n"
     ]
    }
   ],
   "source": [
    "# sentences,score = load_data_from_xml('dataset/financial_posts_ABSA_train.xml')\n",
    "trainX,trainY = load_data_from_file('dataset/final_train.csv')\n",
    "devX,devY = load_data_from_file('dataset/final_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10890, 10890)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX),len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 111)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(devX),len(devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5653, 54)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.count('1'),devY.count('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237, 57)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.count('0'),devY.count('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #only using 1%data for testing code\n",
    "# trainX = trainX[:int(len(trainX)*0.001)]\n",
    "# trainY = trainY[:int(len(trainY)*0.001)]\n",
    "# devX = devX[:int(len(devX)*0.001)]\n",
    "# devY = devY[:int(len(devY)*0.001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", sentence)\n",
    "    #removing stock names to see if it helps\n",
    "    sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train set\n",
      "cleaning dev set\n"
     ]
    }
   ],
   "source": [
    "# extract sentences out of df and cleaning it\n",
    "print('cleaning train set')\n",
    "trainX = [clean_sentence(x) for x in trainX]\n",
    "print('cleaning dev set')\n",
    "devX = [clean_sentence(x) for x in devX]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copying sentences for fastext traning \n",
    "# tranLines = list(trainX)\n",
    "# devLines = list(devX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting real number scores to lables\n",
    "#0-->-ve sentiment 1-->+ve sentiment\n",
    "# labels = [1 if x >= 0 else 0 for x in score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def shuffle_data(sentences,labels,score):\n",
    "#     numbers = [i for i in range(len(sentences))]\n",
    "#     shuffle(numbers)\n",
    "#     temp_text = sentences\n",
    "#     temp_lables = labels\n",
    "#     temp_score = score\n",
    "#     for i in numbers:\n",
    "#         sentences[i] = temp_text[i]\n",
    "#         labels[i]=temp_lables[i]\n",
    "#         score[i] = temp_score[i]\n",
    "#     print(len(sentences))\n",
    "#     print(len(labels))\n",
    "#     print(len(score))\n",
    "#     return sentences,labels,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences,labels,score = shuffle_data(sentences,labels,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doing train and train split\n",
    "# test_train_split_ratio =0.9\n",
    "# trainX,testX = sentences[:int(test_train_split_ratio*len(sentences))],sentences[int(test_train_split_ratio*len(sentences)):]\n",
    "# trainY,testY = labels[:int(test_train_split_ratio*len(labels))],labels[int(test_train_split_ratio*len(labels)):]\n",
    "# score_trainY,score_testY = score[:int(test_train_split_ratio*len(score))],score[int(test_train_split_ratio*len(score)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10890 10890\n",
      "111 111\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX),len(trainY))\n",
    "print(len(devX),len(devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# devY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting output matrix [-ve,+ve]\n",
    "devY = to_categorical(devY,2)\n",
    "trainY = to_categorical(trainY,2)\n",
    "# devY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# devY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 26\n",
      "Vocabulary size: 16423\n",
      "(10890, 26) (111, 26)\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainX)\n",
    "# calculate max document length\n",
    "lengths = [len(s.split()) for s in trainX]\n",
    "max_length = max(lengths)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainX, max_length)\n",
    "devX = encode_text(tokenizer, devX, max_length)\n",
    "print(trainX.shape,devX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0348944 , 0.1041181 , 0.06791693, 0.08504627, 0.04029809,\n",
       "        0.04019213, 0.01059547, 0.00109486, 0.00031786, 0.00014127]),\n",
       " array([ 0. ,  2.6,  5.2,  7.8, 10.4, 13. , 15.6, 18.2, 20.8, 23.4, 26. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAJCCAYAAADUa5GyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f15a58978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(12,10))\n",
    "plt.hist(lengths, normed=True,edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "considring only few sentences have len >20, we can also take max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10890, 26)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index_glove = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "embedding_matrix_glove = get_GloVe_embedding_matrix(embeddings_index_glove)\n",
    "# embedding_matrix[100]\n",
    "# e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')\n",
    "embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "#                 sentence = sentence.encode('UTF-8')\n",
    "#                 print(sentence)\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning new model\n",
      "model trained\n"
     ]
    }
   ],
   "source": [
    "#need to fix this\n",
    "fast_text_model = load_fast_text_model(tranLines+devLines)\n",
    "embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading the model, this can take some time...\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model, this can take some time...\n"
     ]
    }
   ],
   "source": [
    "godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dropout = Real(low=0.4, high=0.9,name = 'dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para_n_dense = Integer(low=100, high=400, name='n_dense')\n",
    "para_n_dense = Categorical(categories=[100,200,300,400], name='n_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para_n_filters = Integer(low=100,high=400,name='n_filters')\n",
    "para_n_filters = Categorical(categories=[100,200,300,400],name='n_filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c1 = Integer(low=1,high=6,name = 'filter_size_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c2 = Integer(low=1,high=6,name = 'filter_size_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c3 = Integer(low=1,high=6,name = 'filter_size_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'embedding_matrix_fast_text',\n",
    "para_em_c1 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_c2 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_c3 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_trainable_flag = Categorical(categories=[True,False],name='em_trainable_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_free_em_dim = Categorical(categories=[100,300,400],name='free_em_dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_batch_size = Categorical(categories=[50,100,150],name='batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ,50,100,200,300,400,500\n",
    "para_epoch = Categorical(categories=[10,50,100,200,300,400,500],name='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size_c1,para_filter_size_c2,para_filter_size_c3,para_em_c1,para_em_c2,para_em_c3,para_em_trainable_flag,para_free_em_dim,para_batch_size,para_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_parameters = [1e-4,0.5,100,100,2,4,6,'embedding_matrix_word2vec','embedding_matrix_word2vec','embedding_matrix_word2vec',False,100,50,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    if em_c1 == 'free':\n",
    "        embedding1 = Embedding(vocab_size, free_em_dim)(inputs1)\n",
    "    else:\n",
    "        embedding1 = Embedding(vocab_size, len(eval(em_c1)[0]), weights = [eval(em_c1)],input_length=length,trainable = em_trainable_flag)(inputs1)\n",
    "\n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size_c1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "#     embedding2 = Embedding(vocab_size, 400, weights = [embedding_matrix_godin],input_length=length,trainable = em_trainable_flag)(inputs2)\n",
    "    if em_c2 == 'free':\n",
    "        embedding2 = Embedding(vocab_size, free_em_dim)(inputs2)\n",
    "    else:\n",
    "        embedding2 = Embedding(vocab_size, len(eval(em_c2)[0]), weights = [eval(em_c2)],input_length=length,trainable = em_trainable_flag)(inputs2)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=filter_size_c2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "#     embedding3 = Embedding(vocab_size, 400)(inputs3)\n",
    "    if em_c3 == 'free':\n",
    "        embedding3 = Embedding(vocab_size, free_em_dim)(inputs3)\n",
    "    else:\n",
    "        embedding3 = Embedding(vocab_size, len(eval(em_c3)[0]), weights = [eval(em_c3)],input_length=length,trainable = em_trainable_flag)(inputs3)\n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=filter_size_c3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(n_dense, activation='relu')(merged)\n",
    "    outputs = Dense(2, activation='softmax')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict to store performance of all models\n",
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,dropout,n_dense,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,em_trainable_flag,free_em_dim,batch_size,epoch):\n",
    "# n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag\n",
    "    # Print the hyper-parameters.\n",
    "    global key\n",
    "    global record\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    print('learning rate ==>',learning_rate)\n",
    "    print('dropout==>',dropout)\n",
    "    print('n_dense==>',n_dense)\n",
    "    print('n_filters==>',n_filters)\n",
    "    print('filter_size_c1',filter_size_c1)\n",
    "    print('filter_size_c2',filter_size_c2)\n",
    "    print('filter_size_c3',filter_size_c3)\n",
    "    print('em_c1==>',em_c1)\n",
    "    print('em_c2==>',em_c2)\n",
    "    print('em_c3==>',em_c3)\n",
    "    print('em_trainable_flag ==>',em_trainable_flag)\n",
    "    print('free_em_dim==>',free_em_dim)\n",
    "    print('batch_size==>',batch_size)\n",
    "    print('epocs==>',epoch)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = define_model(length = max_length,\n",
    "                         vocab_size=vocab_size,\n",
    "                         n_dense=n_dense,\n",
    "                         dropout=dropout,\n",
    "                         learning_rate=learning_rate,\n",
    "                         n_filters=n_filters,\n",
    "                         filter_size_c1=int(filter_size_c1),\n",
    "                         filter_size_c2=int(filter_size_c2),\n",
    "                         filter_size_c3=int(filter_size_c3),\n",
    "                         em_c1=em_c1,\n",
    "                         em_c2=em_c2,\n",
    "                         em_c3=em_c3,\n",
    "                         free_em_dim=free_em_dim,\n",
    "                         em_trainable_flag=em_trainable_flag)\n",
    "\n",
    "    \n",
    "    # Use Keras to train the model.\n",
    "    history_object = model.fit([trainX,trainX,trainX], trainY,epochs=epoch, batch_size=batch_size,validation_data=([devX,devX,devX],devY))\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history_object.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    \n",
    "    \n",
    "    record[key] = {'parameters':[learning_rate,dropout,n_dense,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,em_trainable_flag,free_em_dim,batch_size,epoch],'val_acc':accuracy}\n",
    "    \n",
    "    model.save('models/'+str(key)+'.h5')\n",
    "    \n",
    "    key+=1\n",
    "    \n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=0------------------\n",
      "learning rate ==> 0.0001\n",
      "dropout==> 0.5\n",
      "n_dense==> 100\n",
      "n_filters==> 100\n",
      "filter_size_c1 2\n",
      "filter_size_c2 4\n",
      "filter_size_c3 6\n",
      "em_c1==> embedding_matrix_word2vec\n",
      "em_c2==> embedding_matrix_word2vec\n",
      "em_c3==> embedding_matrix_word2vec\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 100\n",
      "batch_size==> 50\n",
      "epocs==> 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 26, 300)      4926900     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 26, 300)      4926900     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 26, 300)      4926900     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 25, 100)      60100       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 23, 100)      120100      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 21, 100)      180100      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 25, 100)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 23, 100)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 21, 100)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 12, 100)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 11, 100)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 10, 100)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1200)         0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1100)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 1000)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3300)         0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          330100      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            202         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,471,302\n",
      "Trainable params: 690,602\n",
      "Non-trainable params: 14,780,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 10890 samples, validate on 111 samples\n",
      "Epoch 1/10\n",
      "10890/10890 [==============================] - 17s 2ms/step - loss: 0.6695 - acc: 0.5872 - val_loss: 0.6481 - val_acc: 0.6577\n",
      "Epoch 2/10\n",
      " 1700/10890 [===>..........................] - ETA: 14s - loss: 0.6085 - acc: 0.6853"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-f28938caf0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-b9379f69d3e7>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(learning_rate, dropout, n_dense, n_filters, filter_size_c1, filter_size_c2, filter_size_c3, em_c1, em_c2, em_c3, em_trainable_flag, free_em_dim, batch_size, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Use Keras to train the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mhistory_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Get the classification accuracy on the validation-set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=0------------------\n",
      "learning rate ==> 0.0001\n",
      "dropout==> 0.5\n",
      "n_dense==> 100\n",
      "n_filters==> 100\n",
      "filter_size_c1 2\n",
      "filter_size_c2 4\n",
      "filter_size_c3 6\n",
      "em_c1==> embedding_matrix_word2vec\n",
      "em_c2==> embedding_matrix_glove\n",
      "em_c3==> free\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 100\n",
      "batch_size==> 50\n",
      "epocs==> 10\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6986 - acc: 0.5099 - val_loss: 0.7000 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6676 - acc: 0.6023 - val_loss: 0.6957 - val_acc: 0.4375\n",
      "Epoch 3/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6506 - acc: 0.6399 - val_loss: 0.6942 - val_acc: 0.4375\n",
      "Epoch 4/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6258 - acc: 0.6909 - val_loss: 0.6898 - val_acc: 0.4375\n",
      "Epoch 5/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6025 - acc: 0.7106 - val_loss: 0.6755 - val_acc: 0.6250\n",
      "Epoch 6/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.5706 - acc: 0.7368 - val_loss: 0.6791 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.5320 - acc: 0.7712 - val_loss: 0.6807 - val_acc: 0.5625\n",
      "Epoch 8/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.4944 - acc: 0.7967 - val_loss: 0.6608 - val_acc: 0.5625\n",
      "Epoch 9/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.4645 - acc: 0.8177 - val_loss: 0.6732 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.4278 - acc: 0.8407 - val_loss: 0.6630 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n",
      "-----------------------------combination no=1------------------\n",
      "learning rate ==> 0.0001303935287474031\n",
      "dropout==> 0.7881166818054242\n",
      "n_dense==> 200\n",
      "n_filters==> 400\n",
      "filter_size_c1 3\n",
      "filter_size_c2 6\n",
      "filter_size_c3 3\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_word2vec\n",
      "em_c3==> embedding_matrix_godin\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 150\n",
      "epocs==> 8\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/8\n",
      "1569/1569 [==============================] - 12s 8ms/step - loss: 0.7032 - acc: 0.5300 - val_loss: 0.6994 - val_acc: 0.4375\n",
      "Epoch 2/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.6673 - acc: 0.6017 - val_loss: 0.6875 - val_acc: 0.5625\n",
      "Epoch 3/8\n",
      "1569/1569 [==============================] - 12s 8ms/step - loss: 0.6398 - acc: 0.6495 - val_loss: 0.6812 - val_acc: 0.4375\n",
      "Epoch 4/8\n",
      "1569/1569 [==============================] - 13s 8ms/step - loss: 0.6032 - acc: 0.7030 - val_loss: 0.6840 - val_acc: 0.5000\n",
      "Epoch 5/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.5687 - acc: 0.7323 - val_loss: 0.6715 - val_acc: 0.5625\n",
      "Epoch 6/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.5268 - acc: 0.7674 - val_loss: 0.6542 - val_acc: 0.5625\n",
      "Epoch 7/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.4949 - acc: 0.7929 - val_loss: 0.6548 - val_acc: 0.5625\n",
      "Epoch 8/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.4420 - acc: 0.8177 - val_loss: 0.6796 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n",
      "-----------------------------combination no=2------------------\n",
      "learning rate ==> 0.00543087312620765\n",
      "dropout==> 0.8596902940034467\n",
      "n_dense==> 100\n",
      "n_filters==> 400\n",
      "filter_size_c1 4\n",
      "filter_size_c2 5\n",
      "filter_size_c3 4\n",
      "em_c1==> free\n",
      "em_c2==> embedding_matrix_word2vec\n",
      "em_c3==> embedding_matrix_word2vec\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 100\n",
      "epocs==> 10\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 1.0445 - acc: 0.5112 - val_loss: 0.6849 - val_acc: 0.6250\n",
      "Epoch 2/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.6705 - acc: 0.5730 - val_loss: 0.6573 - val_acc: 0.5625\n",
      "Epoch 3/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.3598 - acc: 0.8477 - val_loss: 0.7771 - val_acc: 0.6250\n",
      "Epoch 4/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.1308 - acc: 0.9509 - val_loss: 0.7248 - val_acc: 0.6250\n",
      "Epoch 5/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0659 - acc: 0.9796 - val_loss: 0.8402 - val_acc: 0.6875\n",
      "Epoch 6/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0385 - acc: 0.9892 - val_loss: 1.2451 - val_acc: 0.6875\n",
      "Epoch 7/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0255 - acc: 0.9924 - val_loss: 1.2050 - val_acc: 0.6875\n",
      "Epoch 8/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0181 - acc: 0.9936 - val_loss: 1.4865 - val_acc: 0.6875\n",
      "Epoch 9/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0352 - acc: 0.9885 - val_loss: 1.5816 - val_acc: 0.6250\n",
      "Epoch 10/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0240 - acc: 0.9936 - val_loss: 1.6774 - val_acc: 0.6875\n",
      "Accuracy: 68.75%\n",
      "-----------------------------combination no=3------------------\n",
      "learning rate ==> 0.0008784302296675664\n",
      "dropout==> 0.4109988879439824\n",
      "n_dense==> 400\n",
      "n_filters==> 300\n",
      "filter_size_c1 4\n",
      "filter_size_c2 4\n",
      "filter_size_c3 5\n",
      "em_c1==> embedding_matrix_godin\n",
      "em_c2==> embedding_matrix_fast_text\n",
      "em_c3==> free\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 400\n",
      "batch_size==> 50\n",
      "epocs==> 5\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.7032 - acc: 0.5284 - val_loss: 0.6948 - val_acc: 0.3750\n",
      "Epoch 2/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.5136 - acc: 0.7884 - val_loss: 0.6335 - val_acc: 0.7500\n",
      "Epoch 3/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.1552 - acc: 0.9407 - val_loss: 0.7852 - val_acc: 0.6875\n",
      "Epoch 4/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0425 - acc: 0.9834 - val_loss: 0.8803 - val_acc: 0.5625\n",
      "Epoch 5/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0130 - acc: 0.9975 - val_loss: 1.1222 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n",
      "-----------------------------combination no=4------------------\n",
      "learning rate ==> 0.00944910795479221\n",
      "dropout==> 0.5259951916281641\n",
      "n_dense==> 200\n",
      "n_filters==> 100\n",
      "filter_size_c1 2\n",
      "filter_size_c2 4\n",
      "filter_size_c3 6\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_glove\n",
      "em_c3==> embedding_matrix_fast_text\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 150\n",
      "epocs==> 4\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/4\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 1.5284 - acc: 0.5003 - val_loss: 0.6886 - val_acc: 0.6250\n",
      "Epoch 2/4\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6424 - acc: 0.6201 - val_loss: 0.6769 - val_acc: 0.6250\n",
      "Epoch 3/4\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.4075 - acc: 0.8203 - val_loss: 0.7871 - val_acc: 0.6875\n",
      "Epoch 4/4\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.1520 - acc: 0.9446 - val_loss: 0.7805 - val_acc: 0.6250\n",
      "Accuracy: 62.50%\n",
      "-----------------------------combination no=5------------------\n",
      "learning rate ==> 0.0022466095443656854\n",
      "dropout==> 0.429143995306156\n",
      "n_dense==> 300\n",
      "n_filters==> 300\n",
      "filter_size_c1 6\n",
      "filter_size_c2 3\n",
      "filter_size_c3 4\n",
      "em_c1==> embedding_matrix_godin\n",
      "em_c2==> embedding_matrix_godin\n",
      "em_c3==> embedding_matrix_fast_text\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 100\n",
      "batch_size==> 100\n",
      "epocs==> 3\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/3\n",
      "1569/1569 [==============================] - 6s 4ms/step - loss: 0.7548 - acc: 0.4990 - val_loss: 0.6917 - val_acc: 0.4375\n",
      "Epoch 2/3\n",
      "1569/1569 [==============================] - 5s 3ms/step - loss: 0.6679 - acc: 0.6074 - val_loss: 0.6769 - val_acc: 0.4375\n",
      "Epoch 3/3\n",
      "1569/1569 [==============================] - 5s 3ms/step - loss: 0.5217 - acc: 0.7476 - val_loss: 0.7198 - val_acc: 0.6875\n",
      "Accuracy: 68.75%\n",
      "-----------------------------combination no=6------------------\n",
      "learning rate ==> 0.005365325804556098\n",
      "dropout==> 0.6152436648610217\n",
      "n_dense==> 400\n",
      "n_filters==> 300\n",
      "filter_size_c1 6\n",
      "filter_size_c2 3\n",
      "filter_size_c3 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em_c1==> free\n",
      "em_c2==> embedding_matrix_glove\n",
      "em_c3==> embedding_matrix_glove\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 100\n",
      "epocs==> 3\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/3\n",
      "1569/1569 [==============================] - 7s 4ms/step - loss: 4.3009 - acc: 0.5022 - val_loss: 0.7320 - val_acc: 0.3750\n",
      "Epoch 2/3\n",
      "1569/1569 [==============================] - 8s 5ms/step - loss: 0.7271 - acc: 0.5532 - val_loss: 0.6406 - val_acc: 0.7500\n",
      "Epoch 3/3\n",
      "1569/1569 [==============================] - 8s 5ms/step - loss: 0.5206 - acc: 0.7234 - val_loss: 0.6321 - val_acc: 0.5625\n",
      "Accuracy: 56.25%\n",
      "-----------------------------combination no=7------------------\n",
      "learning rate ==> 0.0006209448862127463\n",
      "dropout==> 0.6674276408699704\n",
      "n_dense==> 400\n",
      "n_filters==> 200\n",
      "filter_size_c1 2\n",
      "filter_size_c2 2\n",
      "filter_size_c3 3\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_fast_text\n",
      "em_c3==> embedding_matrix_godin\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 300\n",
      "batch_size==> 150\n",
      "epocs==> 8\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.7602 - acc: 0.5029 - val_loss: 0.7140 - val_acc: 0.3125\n",
      "Epoch 2/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.6561 - acc: 0.6017 - val_loss: 0.7042 - val_acc: 0.4375\n",
      "Epoch 3/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.6068 - acc: 0.6960 - val_loss: 0.6820 - val_acc: 0.6250\n",
      "Epoch 4/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.5380 - acc: 0.7419 - val_loss: 0.6825 - val_acc: 0.6250\n",
      "Epoch 5/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.4453 - acc: 0.8107 - val_loss: 0.7091 - val_acc: 0.6875\n",
      "Epoch 6/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.3717 - acc: 0.8438 - val_loss: 0.7667 - val_acc: 0.6875\n",
      "Epoch 7/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.3290 - acc: 0.8636 - val_loss: 0.7072 - val_acc: 0.6875\n",
      "Epoch 8/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.2569 - acc: 0.9063 - val_loss: 0.7001 - val_acc: 0.6875\n",
      "Accuracy: 68.75%\n",
      "-----------------------------combination no=8------------------\n",
      "learning rate ==> 0.001158798655778977\n",
      "dropout==> 0.8948764310021187\n",
      "n_dense==> 100\n",
      "n_filters==> 400\n",
      "filter_size_c1 5\n",
      "filter_size_c2 3\n",
      "filter_size_c3 3\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_fast_text\n",
      "em_c3==> embedding_matrix_word2vec\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 300\n",
      "batch_size==> 50\n",
      "epocs==> 8\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/8\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.7251 - acc: 0.5583 - val_loss: 0.6836 - val_acc: 0.5000\n",
      "Epoch 2/8\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.6128 - acc: 0.6871 - val_loss: 0.6931 - val_acc: 0.6875\n",
      "Epoch 3/8\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.5050 - acc: 0.7584 - val_loss: 0.6684 - val_acc: 0.5625\n",
      "Epoch 4/8\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.3962 - acc: 0.8215 - val_loss: 0.5773 - val_acc: 0.7500\n",
      "Epoch 5/8\n",
      "1569/1569 [==============================] - 10s 7ms/step - loss: 0.2553 - acc: 0.8936 - val_loss: 0.6784 - val_acc: 0.6875\n",
      "Epoch 6/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.1942 - acc: 0.9222 - val_loss: 0.7096 - val_acc: 0.6875\n",
      "Epoch 7/8\n",
      "1569/1569 [==============================] - 10s 6ms/step - loss: 0.1169 - acc: 0.9662 - val_loss: 0.8994 - val_acc: 0.6875\n",
      "Epoch 8/8\n",
      "1569/1569 [==============================] - 10s 6ms/step - loss: 0.0916 - acc: 0.9611 - val_loss: 0.9682 - val_acc: 0.6875\n",
      "Accuracy: 68.75%\n",
      "-----------------------------combination no=9------------------\n",
      "learning rate ==> 0.000472944544348041\n",
      "dropout==> 0.6115958972311217\n",
      "n_dense==> 300\n",
      "n_filters==> 400\n",
      "filter_size_c1 6\n",
      "filter_size_c2 2\n",
      "filter_size_c3 3\n",
      "em_c1==> free\n",
      "em_c2==> embedding_matrix_godin\n",
      "em_c3==> free\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 100\n",
      "epocs==> 5\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "1569/1569 [==============================] - 8s 5ms/step - loss: 0.6997 - acc: 0.5118 - val_loss: 0.6982 - val_acc: 0.3750\n",
      "Epoch 2/5\n",
      "1569/1569 [==============================] - 6s 4ms/step - loss: 0.6193 - acc: 0.7266 - val_loss: 0.6425 - val_acc: 0.6250\n",
      "Epoch 3/5\n",
      "1569/1569 [==============================] - 6s 4ms/step - loss: 0.4125 - acc: 0.8693 - val_loss: 0.5552 - val_acc: 0.6250\n",
      "Epoch 4/5\n",
      "1569/1569 [==============================] - 7s 4ms/step - loss: 0.1897 - acc: 0.9433 - val_loss: 0.5841 - val_acc: 0.8125\n",
      "Epoch 5/5\n",
      "1569/1569 [==============================] - 8s 5ms/step - loss: 0.0806 - acc: 0.9726 - val_loss: 0.7572 - val_acc: 0.6250\n",
      "Accuracy: 62.50%\n",
      "-----------------------------combination no=10------------------\n",
      "learning rate ==> 0.0002304493811652546\n",
      "dropout==> 0.8378794135947594\n",
      "n_dense==> 400\n",
      "n_filters==> 300\n",
      "filter_size_c1 2\n",
      "filter_size_c2 3\n",
      "filter_size_c3 2\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_godin\n",
      "em_c3==> embedding_matrix_fast_text\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 300\n",
      "batch_size==> 100\n",
      "epocs==> 3\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/3\n",
      "1569/1569 [==============================] - 6s 4ms/step - loss: 0.7113 - acc: 0.5249 - val_loss: 0.6973 - val_acc: 0.5000\n",
      "Epoch 2/3\n",
      "1569/1569 [==============================] - 4s 3ms/step - loss: 0.6693 - acc: 0.6080 - val_loss: 0.6950 - val_acc: 0.4375\n",
      "Epoch 3/3\n",
      "1569/1569 [==============================] - 5s 3ms/step - loss: 0.6393 - acc: 0.6495 - val_loss: 0.6974 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI',\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.6875,\n",
       "  [0.0006209448862127463,\n",
       "   0.6674276408699704,\n",
       "   400,\n",
       "   200,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'embedding_matrix_godin',\n",
       "   False,\n",
       "   300,\n",
       "   150,\n",
       "   8]),\n",
       " (-0.6875,\n",
       "  [0.001158798655778977,\n",
       "   0.8948764310021187,\n",
       "   100,\n",
       "   400,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'embedding_matrix_word2vec',\n",
       "   True,\n",
       "   300,\n",
       "   50,\n",
       "   8]),\n",
       " (-0.6875,\n",
       "  [0.0022466095443656854,\n",
       "   0.429143995306156,\n",
       "   300,\n",
       "   300,\n",
       "   6,\n",
       "   3,\n",
       "   4,\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   False,\n",
       "   100,\n",
       "   100,\n",
       "   3]),\n",
       " (-0.6875,\n",
       "  [0.00543087312620765,\n",
       "   0.8596902940034467,\n",
       "   100,\n",
       "   400,\n",
       "   4,\n",
       "   5,\n",
       "   4,\n",
       "   'free',\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_word2vec',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   10]),\n",
       " (-0.625,\n",
       "  [0.000472944544348041,\n",
       "   0.6115958972311217,\n",
       "   300,\n",
       "   400,\n",
       "   6,\n",
       "   2,\n",
       "   3,\n",
       "   'free',\n",
       "   'embedding_matrix_godin',\n",
       "   'free',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   5]),\n",
       " (-0.625,\n",
       "  [0.00944910795479221,\n",
       "   0.5259951916281641,\n",
       "   200,\n",
       "   100,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   True,\n",
       "   100,\n",
       "   150,\n",
       "   4]),\n",
       " (-0.5625,\n",
       "  [0.005365325804556098,\n",
       "   0.6152436648610217,\n",
       "   400,\n",
       "   300,\n",
       "   6,\n",
       "   3,\n",
       "   3,\n",
       "   'free',\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_glove',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   3]),\n",
       " (-0.5,\n",
       "  [0.0001,\n",
       "   0.5,\n",
       "   100,\n",
       "   100,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_glove',\n",
       "   'free',\n",
       "   False,\n",
       "   100,\n",
       "   50,\n",
       "   10]),\n",
       " (-0.5,\n",
       "  [0.0001303935287474031,\n",
       "   0.7881166818054242,\n",
       "   200,\n",
       "   400,\n",
       "   3,\n",
       "   6,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_godin',\n",
       "   True,\n",
       "   100,\n",
       "   150,\n",
       "   8]),\n",
       " (-0.5,\n",
       "  [0.0002304493811652546,\n",
       "   0.8378794135947594,\n",
       "   400,\n",
       "   300,\n",
       "   2,\n",
       "   3,\n",
       "   2,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   False,\n",
       "   300,\n",
       "   100,\n",
       "   3]),\n",
       " (-0.5,\n",
       "  [0.0008784302296675664,\n",
       "   0.4109988879439824,\n",
       "   400,\n",
       "   300,\n",
       "   4,\n",
       "   4,\n",
       "   5,\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'free',\n",
       "   False,\n",
       "   400,\n",
       "   50,\n",
       "   5])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'parameters': [0.0001,\n",
       "   0.5,\n",
       "   100,\n",
       "   100,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_glove',\n",
       "   'free',\n",
       "   False,\n",
       "   100,\n",
       "   50,\n",
       "   10],\n",
       "  'val_acc': 0.5},\n",
       " 1: {'parameters': [0.0001303935287474031,\n",
       "   0.7881166818054242,\n",
       "   200,\n",
       "   400,\n",
       "   3,\n",
       "   6,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_godin',\n",
       "   True,\n",
       "   100,\n",
       "   150,\n",
       "   8],\n",
       "  'val_acc': 0.5},\n",
       " 2: {'parameters': [0.00543087312620765,\n",
       "   0.8596902940034467,\n",
       "   100,\n",
       "   400,\n",
       "   4,\n",
       "   5,\n",
       "   4,\n",
       "   'free',\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_word2vec',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   10],\n",
       "  'val_acc': 0.6875},\n",
       " 3: {'parameters': [0.0008784302296675664,\n",
       "   0.4109988879439824,\n",
       "   400,\n",
       "   300,\n",
       "   4,\n",
       "   4,\n",
       "   5,\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'free',\n",
       "   False,\n",
       "   400,\n",
       "   50,\n",
       "   5],\n",
       "  'val_acc': 0.5},\n",
       " 4: {'parameters': [0.00944910795479221,\n",
       "   0.5259951916281641,\n",
       "   200,\n",
       "   100,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   True,\n",
       "   100,\n",
       "   150,\n",
       "   4],\n",
       "  'val_acc': 0.625},\n",
       " 5: {'parameters': [0.0022466095443656854,\n",
       "   0.429143995306156,\n",
       "   300,\n",
       "   300,\n",
       "   6,\n",
       "   3,\n",
       "   4,\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   False,\n",
       "   100,\n",
       "   100,\n",
       "   3],\n",
       "  'val_acc': 0.6875},\n",
       " 6: {'parameters': [0.005365325804556098,\n",
       "   0.6152436648610217,\n",
       "   400,\n",
       "   300,\n",
       "   6,\n",
       "   3,\n",
       "   3,\n",
       "   'free',\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_glove',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   3],\n",
       "  'val_acc': 0.5625},\n",
       " 7: {'parameters': [0.0006209448862127463,\n",
       "   0.6674276408699704,\n",
       "   400,\n",
       "   200,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'embedding_matrix_godin',\n",
       "   False,\n",
       "   300,\n",
       "   150,\n",
       "   8],\n",
       "  'val_acc': 0.6875},\n",
       " 8: {'parameters': [0.001158798655778977,\n",
       "   0.8948764310021187,\n",
       "   100,\n",
       "   400,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'embedding_matrix_word2vec',\n",
       "   True,\n",
       "   300,\n",
       "   50,\n",
       "   8],\n",
       "  'val_acc': 0.6875},\n",
       " 9: {'parameters': [0.000472944544348041,\n",
       "   0.6115958972311217,\n",
       "   300,\n",
       "   400,\n",
       "   6,\n",
       "   2,\n",
       "   3,\n",
       "   'free',\n",
       "   'embedding_matrix_godin',\n",
       "   'free',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   5],\n",
       "  'val_acc': 0.625},\n",
       " 10: {'parameters': [0.0002304493811652546,\n",
       "   0.8378794135947594,\n",
       "   400,\n",
       "   300,\n",
       "   2,\n",
       "   3,\n",
       "   2,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   False,\n",
       "   300,\n",
       "   100,\n",
       "   3],\n",
       "  'val_acc': 0.5}}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record\n",
    "# fit model\n",
    "# history_object = model.fit([trainX,trainX,trainX], trainY,epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('models/record.pickle', 'wb') as handle:\n",
    "    pickle.dump(record, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(12,10))\n",
    "# plt.plot(history_object.history['acc'])\n",
    "# # plt.plot(history_object.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# # plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(12,10))\n",
    "# plt.plot(history_object.history['loss'])\n",
    "# # plt.plot(history_object.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# # plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "# encode data\n",
    "# trainX = encode_text(tokenizer, trainLines, length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # evaluate model on training dataset\n",
    "# loss, acc = model.evaluate([trainX,trainX,trainX], trainY, verbose=0)\n",
    "# print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# # evaluate model on test dataset dataset\n",
    "# loss, acc = model.evaluate([testX,testX,testX], testY, verbose=0)\n",
    "# print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.predict([testX,testX,testX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_predicted_prob(data):\n",
    "#     return [list(x) for x in model.predict(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicted probabilities\n",
    "# train_predicted_prob = get_predicted_prob([trainX,trainX,trainX])\n",
    "# test_predicted_prob = get_predicted_prob([testX,testX,testX])\n",
    "# predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicting lables\n",
    "#if prob>0.5 -->1 else 0\n",
    "# def get_predicted_lables(prob):\n",
    "#     predicted_lables = []\n",
    "#     for x in prob:\n",
    "#         if x>0.5:\n",
    "#             predicted_lables.append(1)\n",
    "#         else:\n",
    "#             predicted_lables.append(0)\n",
    "#     return predicted_lables\n",
    "# def get_predicted_lables(prob):\n",
    "#     predicted_lables = []\n",
    "#     for x in prob:\n",
    "#         if x[0]>x[1]:\n",
    "#             predicted_lables.append(0)\n",
    "#         else:\n",
    "#             predicted_lables.append(1)\n",
    "#     return predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_lables = get_predicted_lables(train_predicted_prob)\n",
    "# test_predicted_lables = get_predicted_lables(test_predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # def get_score_from_prob(prob):\n",
    "# #     m = interp1d([0,1],[-1,1])\n",
    "# #     return [float(m(x)) for x in prob]\n",
    "# def get_score_from_prob(prob):\n",
    "#     intensity_score = []\n",
    "#     for x in prob:\n",
    "#         if x[0]>x[1]:\n",
    "#             intensity_score.append(-1*x[0])\n",
    "#         else:\n",
    "#             intensity_score.append(x[1])\n",
    "#     return intensity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_score = get_score_from_prob(train_predicted_prob)\n",
    "# test_predicted_score = get_score_from_prob(test_predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_rmse = sqrt(mean_squared_error(score_trainY, train_predicted_score))\n",
    "# test_rmse = sqrt(mean_squared_error(score_testY, test_predicted_score))\n",
    "# print(\"(train rmse,test rmse)==\"+str((train_rmse,test_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# foo = [x for x in zip(score_testY,test_predicted_score)]\n",
    "# foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
