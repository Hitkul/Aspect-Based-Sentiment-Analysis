{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi channel CNN for sentiment analysis\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from pickle import dump,load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "def load_file_to_df(filename):\n",
    "    df = pd.read_csv(filename,delimiter='\\t',header=0)\n",
    "    df = df.drop(['Unnamed: 0', 'id'],axis=1)\n",
    "    df_text = df.iloc[:,:1]\n",
    "    df_score = df.iloc[:,1:]\n",
    "    return df_text,df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_text,headlines_score = load_file_to_df(\"FiQA_train_ABSA_financial_headlines.tsv\")\n",
    "post_text,post_score = load_file_to_df(\"FiQA_train_ABSA_financial_posts.tsv\")\n",
    "text = pd.concat([headlines_text,post_text])\n",
    "score = pd.concat([headlines_score,post_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentences out of df and cleaning it\n",
    "sentences = [clean_sentence(x) for x in text['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting real number scores to lables\n",
    "#0-->-ve sentiment 1-->+ve sentiment\n",
    "labels_df = (score>=0).astype(int)\n",
    "labels = [int(x) for x in labels_df['sentiment score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doing train and test split\n",
    "test_train_split_ratio =0.9\n",
    "trainX,testX = sentences[:int(test_train_split_ratio*len(sentences))],sentences[int(test_train_split_ratio*len(sentences)):]\n",
    "trainY,testY = labels[:int(test_train_split_ratio*len(labels))],labels[int(test_train_split_ratio*len(labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3410 3410\n",
      "379 379\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX),len(trainY))\n",
    "print(len(testX),len(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testLines = [' '.join(x) for x in testX]\n",
    "trainLines = [' '.join(x) for x in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainY = np.array(trainY)\n",
    "# testY = np.array(testY)\n",
    "# type(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 23\n",
      "Vocabulary size: 6884\n",
      "(3410, 23)\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           (None, 23)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           (None, 23)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_39 (InputLayer)           (None, 23)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_37 (Embedding)        (None, 23, 100)      688400      input_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_38 (Embedding)        (None, 23, 100)      688400      input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_39 (Embedding)        (None, 23, 100)      688400      input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 20, 32)       12832       embedding_37[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 18, 32)       19232       embedding_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 16, 32)       25632       embedding_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 20, 32)       0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 18, 32)       0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 16, 32)       0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 10, 32)       0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 9, 32)        0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 8, 32)        0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_37 (Flatten)            (None, 320)          0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_38 (Flatten)            (None, 288)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_39 (Flatten)            (None, 256)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 864)          0           flatten_37[0][0]                 \n",
      "                                                                 flatten_38[0][0]                 \n",
      "                                                                 flatten_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 10)           8650        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1)            11          dense_25[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,131,557\n",
      "Trainable params: 2,131,557\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3410/3410 [==============================] - 7s 2ms/step - loss: 0.5834 - acc: 0.6868\n",
      "Epoch 2/10\n",
      "3410/3410 [==============================] - 7s 2ms/step - loss: 0.2380 - acc: 0.9082\n",
      "Epoch 3/10\n",
      "3410/3410 [==============================] - 6s 2ms/step - loss: 0.0992 - acc: 0.9680\n",
      "Epoch 4/10\n",
      "3410/3410 [==============================] - 7s 2ms/step - loss: 0.0705 - acc: 0.9739\n",
      "Epoch 5/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0540 - acc: 0.9777\n",
      "Epoch 6/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0450 - acc: 0.9780\n",
      "Epoch 7/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0347 - acc: 0.9795\n",
      "Epoch 8/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0351 - acc: 0.9771\n",
      "Epoch 9/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0316 - acc: 0.9812\n",
      "Epoch 10/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0312 - acc: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc69286860>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], trainY, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3410, 23) (379, 23)\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "# encode data\n",
    "# trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.328446\n",
      "Test Accuracy: 81.794195\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate([trainX,trainX,trainX], trainY, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([testX,testX,testX], testY, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9988177418708801,\n",
       " 0.26074084639549255,\n",
       " 0.7551729679107666,\n",
       " 0.011647014878690243,\n",
       " 0.0002534111263230443,\n",
       " 0.03343009948730469,\n",
       " 0.997952938079834,\n",
       " 0.999886155128479,\n",
       " 0.28921207785606384,\n",
       " 0.990392804145813,\n",
       " 0.9974619150161743,\n",
       " 0.9999227523803711,\n",
       " 0.0385737307369709,\n",
       " 0.9961506128311157,\n",
       " 0.5448603630065918,\n",
       " 0.4876580536365509,\n",
       " 0.9318739175796509,\n",
       " 0.9997550845146179,\n",
       " 0.9995009899139404,\n",
       " 0.9996243715286255,\n",
       " 0.9676148295402527,\n",
       " 0.5460284352302551,\n",
       " 0.9541715979576111,\n",
       " 0.5444537997245789,\n",
       " 0.7533313035964966,\n",
       " 0.9468314051628113,\n",
       " 0.3868331015110016,\n",
       " 0.999526858329773,\n",
       " 0.9985842704772949,\n",
       " 0.5429437160491943,\n",
       " 0.8434205651283264,\n",
       " 0.5198471546173096,\n",
       " 0.9916826486587524,\n",
       " 0.9839469194412231,\n",
       " 0.1902180165052414,\n",
       " 0.9595324397087097,\n",
       " 0.062798872590065,\n",
       " 0.0004563885449897498,\n",
       " 0.9993263483047485,\n",
       " 0.4437008500099182,\n",
       " 0.6024429202079773,\n",
       " 0.8673034906387329,\n",
       " 0.8393006920814514,\n",
       " 0.9164635539054871,\n",
       " 0.9781244993209839,\n",
       " 0.9540108442306519,\n",
       " 0.9994577765464783,\n",
       " 0.9204396605491638,\n",
       " 0.2716653048992157,\n",
       " 0.0003296658687759191,\n",
       " 0.010836675763130188,\n",
       " 0.3666224777698517,\n",
       " 0.28394925594329834,\n",
       " 0.5687606930732727,\n",
       " 0.9861417412757874,\n",
       " 0.8558883666992188,\n",
       " 0.9985454082489014,\n",
       " 0.9999363422393799,\n",
       " 0.9992828965187073,\n",
       " 0.94978266954422,\n",
       " 0.826072633266449,\n",
       " 0.6239241361618042,\n",
       " 0.9987832903862,\n",
       " 0.3977956473827362,\n",
       " 0.9976354837417603,\n",
       " 3.92489782825578e-05,\n",
       " 0.11392905563116074,\n",
       " 0.628368079662323,\n",
       " 0.00042149453656747937,\n",
       " 0.00020597322145476937,\n",
       " 0.00035607960307970643,\n",
       " 0.0003296658687759191,\n",
       " 0.9983445405960083,\n",
       " 0.9998515844345093,\n",
       " 0.9869220852851868,\n",
       " 0.999951958656311,\n",
       " 0.9999852180480957,\n",
       " 0.00020597322145476937,\n",
       " 0.9997550845146179,\n",
       " 0.962024986743927,\n",
       " 0.9992198944091797,\n",
       " 0.011576080694794655,\n",
       " 0.9811782240867615,\n",
       " 0.6281062364578247,\n",
       " 0.9981301426887512,\n",
       " 0.998555600643158,\n",
       " 2.0507630324573256e-05,\n",
       " 0.02285786159336567,\n",
       " 0.8970512747764587,\n",
       " 0.003709551878273487,\n",
       " 0.1820230484008789,\n",
       " 0.9951759576797485,\n",
       " 0.8164231777191162,\n",
       " 0.00014707722584716976,\n",
       " 0.9998823404312134,\n",
       " 0.6616562604904175,\n",
       " 0.0004001048218924552,\n",
       " 0.9984995126724243,\n",
       " 0.9984995126724243,\n",
       " 0.9988150596618652,\n",
       " 0.999957799911499,\n",
       " 0.999957799911499,\n",
       " 0.9991759657859802,\n",
       " 0.9965135455131531,\n",
       " 6.22158040641807e-05,\n",
       " 0.00020597322145476937,\n",
       " 0.9953822493553162,\n",
       " 0.29007235169410706,\n",
       " 0.8730335831642151,\n",
       " 0.19161607325077057,\n",
       " 0.4676191210746765,\n",
       " 0.00020597322145476937,\n",
       " 0.9999265670776367,\n",
       " 0.0008992947987280786,\n",
       " 0.9997628331184387,\n",
       " 0.9476928114891052,\n",
       " 0.9999775886535645,\n",
       " 0.9999775886535645,\n",
       " 0.9991649389266968,\n",
       " 0.35566216707229614,\n",
       " 0.999957799911499,\n",
       " 0.9996243715286255,\n",
       " 0.0005841458332724869,\n",
       " 0.9999363422393799,\n",
       " 0.999976396560669,\n",
       " 0.999976396560669,\n",
       " 0.9981887936592102,\n",
       " 0.00013329373905435205,\n",
       " 0.0020799697376787663,\n",
       " 0.9997453093528748,\n",
       " 0.0002770212886389345,\n",
       " 0.6648837327957153,\n",
       " 0.9808684587478638,\n",
       " 0.9546325206756592,\n",
       " 0.981681764125824,\n",
       " 0.9952774047851562,\n",
       " 0.9998515844345093,\n",
       " 0.9996685981750488,\n",
       " 0.9969281554222107,\n",
       " 0.9865515232086182,\n",
       " 0.9994969367980957,\n",
       " 0.9615859985351562,\n",
       " 0.9993614554405212,\n",
       " 0.992588222026825,\n",
       " 0.9998911619186401,\n",
       " 0.9893431663513184,\n",
       " 2.0507630324573256e-05,\n",
       " 0.04274873808026314,\n",
       " 0.9927324652671814,\n",
       " 0.00037316043744795024,\n",
       " 1.2076214261469431e-05,\n",
       " 0.9999574422836304,\n",
       " 0.5814937353134155,\n",
       " 0.9996426105499268,\n",
       " 0.00037316043744795024,\n",
       " 0.7017604112625122,\n",
       " 0.9993707537651062,\n",
       " 0.9997550845146179,\n",
       " 0.9999347925186157,\n",
       " 0.9188878536224365,\n",
       " 0.17256756126880646,\n",
       " 0.0005482257110998034,\n",
       " 0.5008331537246704,\n",
       " 0.021815970540046692,\n",
       " 0.17256756126880646,\n",
       " 0.9793624877929688,\n",
       " 0.9572502374649048,\n",
       " 0.8792747259140015,\n",
       " 0.9997628331184387,\n",
       " 4.0396698750555515e-05,\n",
       " 0.844572901725769,\n",
       " 0.999957799911499,\n",
       " 2.0507630324573256e-05,\n",
       " 0.998747706413269,\n",
       " 0.00042151621892116964,\n",
       " 0.9996905326843262,\n",
       " 0.999957799911499,\n",
       " 0.999957799911499,\n",
       " 0.9999852180480957,\n",
       " 0.9962226152420044,\n",
       " 0.9891989827156067,\n",
       " 0.5543186068534851,\n",
       " 0.9993707537651062,\n",
       " 0.0003296658687759191,\n",
       " 0.3454679548740387,\n",
       " 0.00014707722584716976,\n",
       " 0.9996905326843262,\n",
       " 0.9988375306129456,\n",
       " 0.9625096917152405,\n",
       " 0.9999775886535645,\n",
       " 0.9996243715286255,\n",
       " 0.9999775886535645,\n",
       " 0.998396098613739,\n",
       " 0.9540736079216003,\n",
       " 0.9982033967971802,\n",
       " 0.00014707722584716976,\n",
       " 0.00036180036840960383,\n",
       " 2.0507630324573256e-05,\n",
       " 0.00020597322145476937,\n",
       " 0.999951958656311,\n",
       " 0.9997550845146179,\n",
       " 6.22158040641807e-05,\n",
       " 0.9974282383918762,\n",
       " 0.0020799697376787663,\n",
       " 0.9991232752799988,\n",
       " 0.00020597322145476937,\n",
       " 0.999687671661377,\n",
       " 0.2728191018104553,\n",
       " 0.9934112429618835,\n",
       " 0.999687671661377,\n",
       " 0.00049682951066643,\n",
       " 0.00036180036840960383,\n",
       " 0.9986251592636108,\n",
       " 0.999951958656311,\n",
       " 0.9994472861289978,\n",
       " 0.0020799697376787663,\n",
       " 0.0011969871120527387,\n",
       " 0.0002770212886389345,\n",
       " 0.9989203214645386,\n",
       " 0.00049682951066643,\n",
       " 0.00036180036840960383,\n",
       " 0.0002770212886389345,\n",
       " 3.92489782825578e-05,\n",
       " 0.9996905326843262,\n",
       " 0.9992786049842834,\n",
       " 0.998056948184967,\n",
       " 0.9969894289970398,\n",
       " 2.0507630324573256e-05,\n",
       " 0.999976396560669,\n",
       " 2.0507630324573256e-05,\n",
       " 4.0396698750555515e-05,\n",
       " 0.9998711347579956,\n",
       " 0.999886155128479,\n",
       " 0.3217833340167999,\n",
       " 0.980941653251648,\n",
       " 0.10732673853635788,\n",
       " 0.998056948184967,\n",
       " 0.9927840232849121,\n",
       " 0.9850370287895203,\n",
       " 0.9995555281639099,\n",
       " 0.9999227523803711,\n",
       " 0.8244982957839966,\n",
       " 0.9982033967971802,\n",
       " 0.9600160717964172,\n",
       " 0.9996243715286255,\n",
       " 0.9676688313484192,\n",
       " 0.9991878867149353,\n",
       " 0.9787267446517944,\n",
       " 0.9856159687042236,\n",
       " 0.7392557263374329,\n",
       " 0.7527855038642883,\n",
       " 0.4142591953277588,\n",
       " 0.22030135989189148,\n",
       " 0.6239241361618042,\n",
       " 0.9667088985443115,\n",
       " 0.00020597322145476937,\n",
       " 0.9702025651931763,\n",
       " 0.001676986226812005,\n",
       " 0.4794534742832184,\n",
       " 0.993697464466095,\n",
       " 0.999976396560669,\n",
       " 0.9997453093528748,\n",
       " 0.00020597322145476937,\n",
       " 0.999976396560669,\n",
       " 0.999951958656311,\n",
       " 0.9958280920982361,\n",
       " 0.9992198944091797,\n",
       " 0.6281062364578247,\n",
       " 0.9996243715286255,\n",
       " 0.9951759576797485,\n",
       " 0.9991878867149353,\n",
       " 4.0396698750555515e-05,\n",
       " 0.9986251592636108,\n",
       " 0.00020597322145476937,\n",
       " 0.1189662367105484,\n",
       " 0.6693635582923889,\n",
       " 0.9993543028831482,\n",
       " 0.5666421055793762,\n",
       " 0.9711928963661194,\n",
       " 0.7017604112625122,\n",
       " 0.9994577765464783,\n",
       " 0.999687671661377,\n",
       " 0.9999643564224243,\n",
       " 0.9996426105499268,\n",
       " 0.1400446742773056,\n",
       " 0.9903841018676758,\n",
       " 0.3842916190624237,\n",
       " 0.9576206207275391,\n",
       " 0.8473228216171265,\n",
       " 0.4336188733577728,\n",
       " 0.4314001202583313,\n",
       " 0.9903841018676758,\n",
       " 0.5503618121147156,\n",
       " 0.996383786201477,\n",
       " 0.10046029835939407,\n",
       " 0.9968574047088623,\n",
       " 0.999976396560669,\n",
       " 0.9704519510269165,\n",
       " 0.9768021702766418,\n",
       " 0.7513434886932373,\n",
       " 0.9621074199676514,\n",
       " 0.8666707873344421,\n",
       " 0.6538821458816528,\n",
       " 0.0148129528388381,\n",
       " 0.7395939826965332,\n",
       " 0.9991649389266968,\n",
       " 0.044883787631988525,\n",
       " 0.14809010922908783,\n",
       " 0.999886155128479,\n",
       " 0.8894961476325989,\n",
       " 0.9984672665596008,\n",
       " 0.9992189407348633,\n",
       " 0.7613258361816406,\n",
       " 0.9999227523803711,\n",
       " 0.915314257144928,\n",
       " 0.0011775183957070112,\n",
       " 0.00049682951066643,\n",
       " 0.1836709976196289,\n",
       " 0.9995606541633606,\n",
       " 0.0006727864383719862,\n",
       " 0.9996243715286255,\n",
       " 0.9652697443962097,\n",
       " 0.9993707537651062,\n",
       " 0.9980890154838562,\n",
       " 0.9991878867149353,\n",
       " 0.999526858329773,\n",
       " 0.996105968952179,\n",
       " 0.9977306723594666,\n",
       " 0.9988313317298889,\n",
       " 0.9969713687896729,\n",
       " 0.9930307269096375,\n",
       " 0.4580943286418915,\n",
       " 0.0006173625006340444,\n",
       " 0.913677453994751,\n",
       " 0.11378587782382965,\n",
       " 0.7392557263374329,\n",
       " 0.002157818293198943,\n",
       " 0.3974526822566986,\n",
       " 0.0005400020745582879,\n",
       " 0.05813879147171974,\n",
       " 0.8996369242668152,\n",
       " 0.0004563885449897498,\n",
       " 0.9729971289634705,\n",
       " 0.7618804574012756,\n",
       " 0.7527855038642883,\n",
       " 0.9995713829994202,\n",
       " 0.9475806951522827,\n",
       " 0.9084783792495728,\n",
       " 0.3921841084957123,\n",
       " 0.00036180036840960383,\n",
       " 0.6103085875511169,\n",
       " 0.9988454580307007,\n",
       " 0.9994577765464783,\n",
       " 0.9204396605491638,\n",
       " 0.8497380614280701,\n",
       " 0.9995239973068237,\n",
       " 0.8250855207443237,\n",
       " 0.4486044943332672,\n",
       " 0.9985454082489014,\n",
       " 0.0002770212886389345,\n",
       " 0.9992828965187073,\n",
       " 0.9936902523040771,\n",
       " 0.6239241361618042,\n",
       " 0.9036766290664673,\n",
       " 0.9527617692947388,\n",
       " 0.9970362186431885,\n",
       " 0.4195360243320465,\n",
       " 0.9997673630714417,\n",
       " 0.9257357120513916,\n",
       " 0.9652697443962097,\n",
       " 0.9999173879623413,\n",
       " 0.9158214926719666,\n",
       " 0.9803915023803711,\n",
       " 0.9997453093528748,\n",
       " 0.9999775886535645,\n",
       " 0.9999775886535645,\n",
       " 0.9999363422393799,\n",
       " 0.9751960039138794,\n",
       " 0.24132873117923737]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_prob = [float(x) for x in model.predict([testX,testX,testX])]\n",
    "predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lables = []\n",
    "for x in predicted_prob:\n",
    "    if x>0.5:\n",
    "        predicted_lables.append(1)\n",
    "    else:\n",
    "        predicted_lables.append(0)\n",
    "predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
