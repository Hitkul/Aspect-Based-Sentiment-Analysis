{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi channel CNN for sentiment analysis\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from pickle import dump,load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "def load_file_to_df(filename):\n",
    "    df = pd.read_csv(filename,delimiter='\\t',header=0)\n",
    "    df = df.drop(['Unnamed: 0', 'id'],axis=1)\n",
    "    df_text = df.iloc[:,:1]\n",
    "    df_score = df.iloc[:,1:]\n",
    "    return df_text,df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_text,headlines_score = load_file_to_df(\"FiQA_train_ABSA_financial_headlines.tsv\")\n",
    "post_text,post_score = load_file_to_df(\"FiQA_train_ABSA_financial_posts.tsv\")\n",
    "text = pd.concat([headlines_text,post_text])\n",
    "score = pd.concat([headlines_score,post_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", sentence)\n",
    "    #removing stock names to see if it helps\n",
    "    sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentences out of df and cleaning it\n",
    "sentences = [clean_sentence(x) for x in text['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting real number scores to lables\n",
    "#0-->-ve sentiment 1-->+ve sentiment\n",
    "labels_df = (score>=0).astype(int)\n",
    "labels = [int(x) for x in labels_df['sentiment score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling dataset\n",
    "numbers = [i for i in range(len(sentences))]\n",
    "shuffle(numbers)\n",
    "# numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3789\n",
      "3789\n"
     ]
    }
   ],
   "source": [
    "temp_text = sentences\n",
    "temp_lables = labels\n",
    "for i in numbers:\n",
    "    sentences[i] = temp_text[i]\n",
    "    labels[i]=temp_lables[i]\n",
    "print(len(sentences))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doing train and test split\n",
    "test_train_split_ratio =0.9\n",
    "trainX,testX = sentences[:int(test_train_split_ratio*len(sentences))],sentences[int(test_train_split_ratio*len(sentences)):]\n",
    "trainY,testY = labels[:int(test_train_split_ratio*len(labels))],labels[int(test_train_split_ratio*len(labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3410 3410\n",
      "379 379\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX),len(trainY))\n",
    "print(len(testX),len(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testLines = [' '.join(x) for x in testX]\n",
    "trainLines = [' '.join(x) for x in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainY = np.array(trainY)\n",
    "# testY = np.array(testY)\n",
    "# type(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 21\n",
      "Vocabulary size: 6019\n",
      "(3410, 21)\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_61 (InputLayer)           (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_62 (InputLayer)           (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_63 (InputLayer)           (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_61 (Embedding)        (None, 21, 100)      601900      input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_62 (Embedding)        (None, 21, 100)      601900      input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_63 (Embedding)        (None, 21, 100)      601900      input_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 18, 32)       12832       embedding_61[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 16, 32)       19232       embedding_62[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 14, 32)       25632       embedding_63[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 18, 32)       0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 16, 32)       0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 14, 32)       0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 9, 32)        0           dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 8, 32)        0           dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 7, 32)        0           dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_61 (Flatten)            (None, 288)          0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_62 (Flatten)            (None, 256)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_63 (Flatten)            (None, 224)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 768)          0           flatten_61[0][0]                 \n",
      "                                                                 flatten_62[0][0]                 \n",
      "                                                                 flatten_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 10)           7690        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 1)            11          dense_41[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,871,097\n",
      "Trainable params: 1,871,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3410/3410 [==============================] - 7s 2ms/step - loss: 0.6093 - acc: 0.6578\n",
      "Epoch 2/10\n",
      "3410/3410 [==============================] - 6s 2ms/step - loss: 0.2747 - acc: 0.8930\n",
      "Epoch 3/10\n",
      "3410/3410 [==============================] - 6s 2ms/step - loss: 0.1177 - acc: 0.9619\n",
      "Epoch 4/10\n",
      "3410/3410 [==============================] - 6s 2ms/step - loss: 0.0789 - acc: 0.9736\n",
      "Epoch 5/10\n",
      "3410/3410 [==============================] - 6s 2ms/step - loss: 0.0602 - acc: 0.9780\n",
      "Epoch 6/10\n",
      "3410/3410 [==============================] - 6s 2ms/step - loss: 0.0535 - acc: 0.9760\n",
      "Epoch 7/10\n",
      "3344/3410 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9755"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], trainY, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3410, 21) (379, 21)\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "# encode data\n",
    "# trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.299120\n",
      "Test Accuracy: 81.794195\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate([trainX,trainX,trainX], trainY, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([testX,testX,testX], testY, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9087198972702026,\n",
       " 0.0632719099521637,\n",
       " 0.2494022250175476,\n",
       " 0.009136034175753593,\n",
       " 0.003522432642057538,\n",
       " 0.012840733863413334,\n",
       " 0.9890540838241577,\n",
       " 0.9976258873939514,\n",
       " 0.5278922915458679,\n",
       " 0.9804621934890747,\n",
       " 0.9933223128318787,\n",
       " 0.9999707937240601,\n",
       " 0.013825641013681889,\n",
       " 0.5195249319076538,\n",
       " 0.23805579543113708,\n",
       " 0.1154065653681755,\n",
       " 0.9212626814842224,\n",
       " 0.999370276927948,\n",
       " 0.998174786567688,\n",
       " 0.9788433313369751,\n",
       " 0.9664455056190491,\n",
       " 0.16857567429542542,\n",
       " 0.824804425239563,\n",
       " 0.5600648522377014,\n",
       " 0.08045078068971634,\n",
       " 0.2489846646785736,\n",
       " 0.3578241765499115,\n",
       " 0.9988269209861755,\n",
       " 0.9989207983016968,\n",
       " 0.06143384054303169,\n",
       " 0.5046038627624512,\n",
       " 0.05657781660556793,\n",
       " 0.8836639523506165,\n",
       " 0.9080017805099487,\n",
       " 0.08116249740123749,\n",
       " 0.7313534021377563,\n",
       " 0.18947917222976685,\n",
       " 0.0026434564497321844,\n",
       " 0.9889487028121948,\n",
       " 0.08191819489002228,\n",
       " 0.255705326795578,\n",
       " 0.648998498916626,\n",
       " 0.05936622992157936,\n",
       " 0.04358725994825363,\n",
       " 0.9739857316017151,\n",
       " 0.16010360419750214,\n",
       " 0.997700035572052,\n",
       " 0.26341527700424194,\n",
       " 0.14031432569026947,\n",
       " 0.003988571465015411,\n",
       " 0.01327225100249052,\n",
       " 0.06975168734788895,\n",
       " 0.008562762290239334,\n",
       " 0.6061182618141174,\n",
       " 0.9666162729263306,\n",
       " 0.9163697361946106,\n",
       " 0.9926194548606873,\n",
       " 0.9999825954437256,\n",
       " 0.9986862540245056,\n",
       " 0.9989792704582214,\n",
       " 0.8879795670509338,\n",
       " 0.4543133080005646,\n",
       " 0.9929614663124084,\n",
       " 0.14796942472457886,\n",
       " 0.9947375655174255,\n",
       " 0.001964919501915574,\n",
       " 0.042844295501708984,\n",
       " 0.9164640307426453,\n",
       " 0.005792959127575159,\n",
       " 0.0037859627045691013,\n",
       " 0.00506770983338356,\n",
       " 0.003988571465015411,\n",
       " 0.9944193959236145,\n",
       " 0.9981607794761658,\n",
       " 0.9265398979187012,\n",
       " 0.9998492002487183,\n",
       " 0.9998205304145813,\n",
       " 0.0037859627045691013,\n",
       " 0.999370276927948,\n",
       " 0.43878471851348877,\n",
       " 0.9664043188095093,\n",
       " 0.023651909083127975,\n",
       " 0.9776207804679871,\n",
       " 0.18075108528137207,\n",
       " 0.9923661351203918,\n",
       " 0.9992318153381348,\n",
       " 0.002047391375526786,\n",
       " 0.007709649857133627,\n",
       " 0.552347719669342,\n",
       " 0.6955302357673645,\n",
       " 0.19197212159633636,\n",
       " 0.9822813272476196,\n",
       " 0.8075050711631775,\n",
       " 0.008344304747879505,\n",
       " 0.9998763799667358,\n",
       " 0.03904731199145317,\n",
       " 0.0038644885644316673,\n",
       " 0.9927600622177124,\n",
       " 0.9927600622177124,\n",
       " 0.9988728165626526,\n",
       " 0.9998685121536255,\n",
       " 0.9998685121536255,\n",
       " 0.9970812201499939,\n",
       " 0.9955862760543823,\n",
       " 0.0016970621654763818,\n",
       " 0.0037859627045691013,\n",
       " 0.9663711786270142,\n",
       " 0.04666682705283165,\n",
       " 0.4456295967102051,\n",
       " 0.06693639606237411,\n",
       " 0.1154065653681755,\n",
       " 0.0037859627045691013,\n",
       " 0.999981164932251,\n",
       " 0.00502913910895586,\n",
       " 0.9986565113067627,\n",
       " 0.7458157539367676,\n",
       " 0.9998767375946045,\n",
       " 0.9998767375946045,\n",
       " 0.9966557025909424,\n",
       " 0.360373318195343,\n",
       " 0.9998685121536255,\n",
       " 0.9788433313369751,\n",
       " 0.004175352398306131,\n",
       " 0.9999825954437256,\n",
       " 0.9992054104804993,\n",
       " 0.9992054104804993,\n",
       " 0.9959110021591187,\n",
       " 0.005966991186141968,\n",
       " 0.012898799031972885,\n",
       " 0.9994775652885437,\n",
       " 0.007208217866718769,\n",
       " 0.7233564257621765,\n",
       " 0.9715840220451355,\n",
       " 0.9832846522331238,\n",
       " 0.22958500683307648,\n",
       " 0.8968797922134399,\n",
       " 0.9981607794761658,\n",
       " 0.9921736717224121,\n",
       " 0.9969360828399658,\n",
       " 0.8256781697273254,\n",
       " 0.9959854483604431,\n",
       " 0.34314024448394775,\n",
       " 0.9976235032081604,\n",
       " 0.5312137007713318,\n",
       " 0.9995104074478149,\n",
       " 0.9250453114509583,\n",
       " 0.002047391375526786,\n",
       " 0.020773980766534805,\n",
       " 0.9921941757202148,\n",
       " 0.003088289638981223,\n",
       " 0.0025335007812827826,\n",
       " 0.999355137348175,\n",
       " 0.2519797682762146,\n",
       " 0.9980813264846802,\n",
       " 0.003088289638981223,\n",
       " 0.6640940308570862,\n",
       " 0.996664822101593,\n",
       " 0.999370276927948,\n",
       " 0.9996391534805298,\n",
       " 0.9033811092376709,\n",
       " 0.7813075184822083,\n",
       " 0.002955347066745162,\n",
       " 0.5570659637451172,\n",
       " 0.19571475684642792,\n",
       " 0.7813075184822083,\n",
       " 0.9466249942779541,\n",
       " 0.8104715943336487,\n",
       " 0.9510545134544373,\n",
       " 0.9986565113067627,\n",
       " 0.0031626324634999037,\n",
       " 0.8051590323448181,\n",
       " 0.9998685121536255,\n",
       " 0.002047391375526786,\n",
       " 0.9828099012374878,\n",
       " 0.003301563672721386,\n",
       " 0.9955222606658936,\n",
       " 0.9998685121536255,\n",
       " 0.9998685121536255,\n",
       " 0.9998205304145813,\n",
       " 0.9998226761817932,\n",
       " 0.9965276122093201,\n",
       " 0.3048388659954071,\n",
       " 0.996664822101593,\n",
       " 0.003988571465015411,\n",
       " 0.1522780805826187,\n",
       " 0.008344304747879505,\n",
       " 0.9955222606658936,\n",
       " 0.9996086955070496,\n",
       " 0.3088890612125397,\n",
       " 0.9998767375946045,\n",
       " 0.9788433313369751,\n",
       " 0.9998767375946045,\n",
       " 0.9980332255363464,\n",
       " 0.7699839472770691,\n",
       " 0.9995864033699036,\n",
       " 0.008344304747879505,\n",
       " 0.004965497646480799,\n",
       " 0.002047391375526786,\n",
       " 0.0037859627045691013,\n",
       " 0.9998492002487183,\n",
       " 0.999370276927948,\n",
       " 0.0016970621654763818,\n",
       " 0.9953757524490356,\n",
       " 0.012898799031972885,\n",
       " 0.9909049868583679,\n",
       " 0.0037859627045691013,\n",
       " 0.9992097616195679,\n",
       " 0.3067260682582855,\n",
       " 0.9819981455802917,\n",
       " 0.9992097616195679,\n",
       " 0.005691991653293371,\n",
       " 0.004965497646480799,\n",
       " 0.9980378746986389,\n",
       " 0.9998492002487183,\n",
       " 0.9997498393058777,\n",
       " 0.012898799031972885,\n",
       " 0.00761839747428894,\n",
       " 0.007208217866718769,\n",
       " 0.9575936794281006,\n",
       " 0.005691991653293371,\n",
       " 0.004965497646480799,\n",
       " 0.007208217866718769,\n",
       " 0.001964919501915574,\n",
       " 0.9955222606658936,\n",
       " 0.9989125728607178,\n",
       " 0.9892443418502808,\n",
       " 0.9865574240684509,\n",
       " 0.002047391375526786,\n",
       " 0.9992054104804993,\n",
       " 0.002047391375526786,\n",
       " 0.0031626324634999037,\n",
       " 0.9996629953384399,\n",
       " 0.9976258873939514,\n",
       " 0.3007561266422272,\n",
       " 0.9967004656791687,\n",
       " 0.0025537617038935423,\n",
       " 0.9892443418502808,\n",
       " 0.9936425685882568,\n",
       " 0.9137812852859497,\n",
       " 0.9991472959518433,\n",
       " 0.9999707937240601,\n",
       " 0.7044642567634583,\n",
       " 0.9995864033699036,\n",
       " 0.9077041745185852,\n",
       " 0.9788433313369751,\n",
       " 0.9887304902076721,\n",
       " 0.9980736970901489,\n",
       " 0.9254599809646606,\n",
       " 0.9039652943611145,\n",
       " 0.8668097853660583,\n",
       " 0.03117954544723034,\n",
       " 0.14428068697452545,\n",
       " 0.15281452238559723,\n",
       " 0.4543133080005646,\n",
       " 0.9808915257453918,\n",
       " 0.0037859627045691013,\n",
       " 0.7275918126106262,\n",
       " 0.009799613617360592,\n",
       " 0.4020078480243683,\n",
       " 0.9804868102073669,\n",
       " 0.9992054104804993,\n",
       " 0.9994775652885437,\n",
       " 0.0037859627045691013,\n",
       " 0.9992054104804993,\n",
       " 0.9998492002487183,\n",
       " 0.9901930093765259,\n",
       " 0.9664043188095093,\n",
       " 0.18075108528137207,\n",
       " 0.9788433313369751,\n",
       " 0.9822813272476196,\n",
       " 0.9980736970901489,\n",
       " 0.0031626324634999037,\n",
       " 0.9980378746986389,\n",
       " 0.0037859627045691013,\n",
       " 0.04254373162984848,\n",
       " 0.5015149712562561,\n",
       " 0.9985766410827637,\n",
       " 0.1065886989235878,\n",
       " 0.6105409264564514,\n",
       " 0.6640940308570862,\n",
       " 0.997700035572052,\n",
       " 0.9992097616195679,\n",
       " 0.9996022582054138,\n",
       " 0.9980813264846802,\n",
       " 0.03371046483516693,\n",
       " 0.8376143574714661,\n",
       " 0.07779445499181747,\n",
       " 0.9330462217330933,\n",
       " 0.46409761905670166,\n",
       " 0.48962146043777466,\n",
       " 0.24240337312221527,\n",
       " 0.8376143574714661,\n",
       " 0.16881301999092102,\n",
       " 0.9963035583496094,\n",
       " 0.00641105230897665,\n",
       " 0.9438611268997192,\n",
       " 0.9992054104804993,\n",
       " 0.9496666789054871,\n",
       " 0.5301713347434998,\n",
       " 0.8677067756652832,\n",
       " 0.6740896105766296,\n",
       " 0.042090870440006256,\n",
       " 0.2911674678325653,\n",
       " 0.011457757093012333,\n",
       " 0.2514974772930145,\n",
       " 0.9966557025909424,\n",
       " 0.02950288914144039,\n",
       " 0.2946265637874603,\n",
       " 0.9976258873939514,\n",
       " 0.060717400163412094,\n",
       " 0.9840279221534729,\n",
       " 0.9929205775260925,\n",
       " 0.6190903782844543,\n",
       " 0.9999707937240601,\n",
       " 0.19661559164524078,\n",
       " 0.005094660446047783,\n",
       " 0.005691991653293371,\n",
       " 0.08928874135017395,\n",
       " 0.99779212474823,\n",
       " 0.004675081465393305,\n",
       " 0.9788433313369751,\n",
       " 0.9795039296150208,\n",
       " 0.996664822101593,\n",
       " 0.9918338060379028,\n",
       " 0.9980736970901489,\n",
       " 0.9988269209861755,\n",
       " 0.9712915420532227,\n",
       " 0.9980100989341736,\n",
       " 0.9968885779380798,\n",
       " 0.9856712222099304,\n",
       " 0.6791594624519348,\n",
       " 0.8203548192977905,\n",
       " 0.00880675483494997,\n",
       " 0.9443759322166443,\n",
       " 0.00907791405916214,\n",
       " 0.8668097853660583,\n",
       " 0.004942005034536123,\n",
       " 0.1981065571308136,\n",
       " 0.004063233733177185,\n",
       " 0.025470154359936714,\n",
       " 0.7917213439941406,\n",
       " 0.0026434564497321844,\n",
       " 0.9786132574081421,\n",
       " 0.16363206505775452,\n",
       " 0.03117954544723034,\n",
       " 0.999683141708374,\n",
       " 0.704206645488739,\n",
       " 0.5310620069503784,\n",
       " 0.0943906232714653,\n",
       " 0.004965497646480799,\n",
       " 0.168660506606102,\n",
       " 0.8995919823646545,\n",
       " 0.997700035572052,\n",
       " 0.26341527700424194,\n",
       " 0.11670644581317902,\n",
       " 0.9965478777885437,\n",
       " 0.9542529582977295,\n",
       " 0.1880636066198349,\n",
       " 0.9926194548606873,\n",
       " 0.007208217866718769,\n",
       " 0.9986862540245056,\n",
       " 0.9329951405525208,\n",
       " 0.4543133080005646,\n",
       " 0.3673126995563507,\n",
       " 0.9582041501998901,\n",
       " 0.9889515042304993,\n",
       " 0.26502516865730286,\n",
       " 0.998431384563446,\n",
       " 0.37668001651763916,\n",
       " 0.9795039296150208,\n",
       " 0.9998728036880493,\n",
       " 0.27452707290649414,\n",
       " 0.5061993598937988,\n",
       " 0.9994775652885437,\n",
       " 0.9998767375946045,\n",
       " 0.9998767375946045,\n",
       " 0.9999825954437256,\n",
       " 0.9039912223815918,\n",
       " 0.11540666967630386]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_prob = [float(x) for x in model.predict([testX,testX,testX])]\n",
    "predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lables = []\n",
    "for x in predicted_prob:\n",
    "    if x>0.5:\n",
    "        predicted_lables.append(1)\n",
    "    else:\n",
    "        predicted_lables.append(0)\n",
    "predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
