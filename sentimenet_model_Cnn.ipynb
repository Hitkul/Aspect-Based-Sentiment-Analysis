{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi channel CNN for sentiment analysis\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import word2vecReader as godin_embedding\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from gensim.models import KeyedVectors\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "def load_data_from_file(filename):\n",
    "    print(\"loading file = \",filename)\n",
    "    sentences = []\n",
    "    label = []\n",
    "    with codecs.open(filename, \"r\",encoding='utf-8', errors='ignore') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            try:\n",
    "                sentences.append(row[0])\n",
    "                label.append(row[1])\n",
    "            except:\n",
    "                print(row)\n",
    "    return sentences,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file =  dataset/final_train.csv\n",
      "loading file =  dataset/final_dev.csv\n"
     ]
    }
   ],
   "source": [
    "# sentences,score = load_data_from_xml('dataset/financial_posts_ABSA_train.xml')\n",
    "trainX,trainY = load_data_from_file('dataset/final_train.csv')\n",
    "devX,devY = load_data_from_file('dataset/final_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10890, 10890)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX),len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 111)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(devX),len(devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5653, 54)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.count('1'),devY.count('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237, 57)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.count('0'),devY.count('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #only using 1%data for testing code\n",
    "trainX = trainX[:100]\n",
    "trainY = trainY[:100]\n",
    "devX = devX[:10]\n",
    "devY = devY[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #removing stock names to see if it helps\n",
    "#     sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation.replace('$',''))\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "#     remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train set\n",
      "cleaning dev set\n"
     ]
    }
   ],
   "source": [
    "# extract sentences out of df and cleaning it\n",
    "print('cleaning train set')\n",
    "trainX = [clean_sentence(x) for x in trainX]\n",
    "print('cleaning dev set')\n",
    "devX = [clean_sentence(x) for x in devX]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX),len(trainY))\n",
    "print(len(devX),len(devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting output matrix [-ve,+ve]\n",
    "devY = to_categorical(devY,2)\n",
    "trainY = to_categorical(trainY,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = [len(s.split()) for s in trainX]\n",
    "max_length = max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.1       , 0.06428571, 0.14285714, 0.03571429, 0.07857143,\n",
       "        0.12857143, 0.06428571, 0.02857143, 0.02857143, 0.04285714]),\n",
       " array([ 2. ,  3.4,  4.8,  6.2,  7.6,  9. , 10.4, 11.8, 13.2, 14.6, 16. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAJCCAYAAADUa5GyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f114647ddd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(12,10))\n",
    "plt.hist(lengths, normed=True,edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading the model, this can take some time...\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model, this can take some time...\n"
     ]
    }
   ],
   "source": [
    "godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(model,sentence,godin_flag = False):\n",
    "    tokens = sentence.split()[:max_length]\n",
    "    if godin_flag:\n",
    "        embedding_matrix = np.zeros((max_length,400))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((max_length,300))\n",
    "    for i,word in enumerate(tokens):\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulding word2vec matrix of train set\n",
      "bulding godin matrix of train set\n",
      "bulding word2vec matrix of dev set\n",
      "bulding godin matrix of dev set\n"
     ]
    }
   ],
   "source": [
    "print(\"bulding word2vec matrix of train set\")\n",
    "train_word2vec = np.asarray([get_embedding_matrix(word2vec_model,x) for x in trainX])\n",
    "print(\"bulding godin matrix of train set\")\n",
    "train_godin = np.asarray([get_embedding_matrix(godin_model,x,godin_flag=True) for x in trainX])\n",
    "print(\"bulding word2vec matrix of dev set\")\n",
    "dev_word2vec = np.asarray([get_embedding_matrix(word2vec_model,x) for x in devX])\n",
    "print(\"bulding godin matrix of dev set\")\n",
    "dev_godin = np.asarray([get_embedding_matrix(godin_model,x,godin_flag=True) for x in devX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 16, 300)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dropout = Real(low=0.4, high=0.9,name = 'dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para_n_dense = Integer(low=100, high=400, name='n_dense')\n",
    "para_n_dense = Categorical(categories=[100,200,300,400], name='n_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para_n_filters = Integer(low=100,high=400,name='n_filters')\n",
    "para_n_filters = Categorical(categories=[100,200,300,400],name='n_filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c1 = Integer(low=1,high=6,name = 'filter_size_c1')\n",
    "para_filter_size_c2 = Integer(low=1,high=6,name = 'filter_size_c2')\n",
    "para_filter_size_c3 = Integer(low=1,high=6,name = 'filter_size_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_c1 = Categorical(categories=['embedding_matrix_godin','embedding_matrix_word2vec'],name='em_c1')\n",
    "para_em_c2 = Categorical(categories=['embedding_matrix_godin','embedding_matrix_word2vec'],name='em_c2')\n",
    "para_em_c3 = Categorical(categories=['embedding_matrix_godin','embedding_matrix_word2vec'],name='em_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_batch_size = Categorical(categories=[50,100,150],name='batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_epoch = Categorical(categories=[10,50,100,200,300,400,500],name='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size_c1,para_filter_size_c2,para_filter_size_c3,para_em_c1,para_em_c2,para_em_c3,para_batch_size,para_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_parameters = [1e-4,0.5,100,100,2,4,6,'embedding_matrix_word2vec','embedding_matrix_godin','embedding_matrix_word2vec',50,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(length,n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3):\n",
    "    # channel 1\n",
    "    if em_c1 == 'embedding_matrix_word2vec':\n",
    "        inputs1 = Input(shape=(length,300))\n",
    "    else:\n",
    "        inputs1 = Input(shape=(length,400))\n",
    "#     if em_c1 == 'free':\n",
    "#         embedding1 = Embedding(vocab_size, free_em_dim)(inputs1)\n",
    "#     else:\n",
    "#         embedding1 = Embedding(vocab_size, len(eval(em_c1)[0]), weights = [eval(em_c1)],input_length=length,trainable = em_trainable_flag)(inputs1)\n",
    "\n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size_c1, activation='relu')(inputs1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    if em_c2 == 'embedding_matrix_word2vec':\n",
    "        inputs2 = Input(shape=(length,300))\n",
    "    else:\n",
    "        inputs2 = Input(shape=(length,400))\n",
    "#     embedding2 = Embedding(vocab_size, 400, weights = [embedding_matrix_godin],input_length=length,trainable = em_trainable_flag)(inputs2)\n",
    "#     if em_c2 == 'free':\n",
    "#         embedding2 = Embedding(vocab_size, free_em_dim)(inputs2)\n",
    "#     else:\n",
    "#         embedding2 = Embedding(vocab_size, len(eval(em_c2)[0]), weights = [eval(em_c2)],input_length=length,trainable = em_trainable_flag)(inputs2)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=filter_size_c2, activation='relu')(inputs2)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    if em_c1 == 'embedding_matrix_word2vec':\n",
    "        inputs3 = Input(shape=(length,300))\n",
    "    else:\n",
    "        inputs3 = Input(shape=(length,400))\n",
    "#     embedding3 = Embedding(vocab_size, 400)(inputs3)\n",
    "#     if em_c3 == 'free':\n",
    "#         embedding3 = Embedding(vocab_size, free_em_dim)(inputs3)\n",
    "#     else:\n",
    "#         embedding3 = Embedding(vocab_size, len(eval(em_c3)[0]), weights = [eval(em_c3)],input_length=length,trainable = em_trainable_flag)(inputs3)\n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=filter_size_c3, activation='relu')(inputs3)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(n_dense, activation='relu')(merged)\n",
    "    outputs = Dense(2, activation='softmax')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict to store performance of all models\n",
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,dropout,n_dense,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,batch_size,epoch):\n",
    "# n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag\n",
    "    # Print the hyper-parameters.\n",
    "    global key\n",
    "    global record\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    print('learning rate ==>',learning_rate)\n",
    "    print('dropout==>',dropout)\n",
    "    print('n_dense==>',n_dense)\n",
    "    print('n_filters==>',n_filters)\n",
    "    print('filter_size_c1',filter_size_c1)\n",
    "    print('filter_size_c2',filter_size_c2)\n",
    "    print('filter_size_c3',filter_size_c3)\n",
    "    print('em_c1==>',em_c1)\n",
    "    print('em_c2==>',em_c2)\n",
    "    print('em_c3==>',em_c3)\n",
    "    print('batch_size==>',batch_size)\n",
    "    print('epocs==>',epoch)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = define_model(length = max_length,\n",
    "                         n_dense=n_dense,\n",
    "                         dropout=dropout,\n",
    "                         learning_rate=learning_rate,\n",
    "                         n_filters=n_filters,\n",
    "                         filter_size_c1=int(filter_size_c1),\n",
    "                         filter_size_c2=int(filter_size_c2),\n",
    "                         filter_size_c3=int(filter_size_c3),\n",
    "                         em_c1=em_c1,\n",
    "                         em_c2=em_c2,\n",
    "                         em_c3=em_c3)\n",
    "    input_train_array = [train_word2vec if x=='embedding_matrix_word2vec' else train_godin for x in [em_c1,em_c2,em_c3]]\n",
    "    input_dev_array = [dev_word2vec if x=='embedding_matrix_word2vec' else dev_godin for x in [em_c1,em_c2,em_c3]]\n",
    "    \n",
    "    # Use Keras to train the model.\n",
    "    history_object = model.fit(input_train_array, trainY,epochs=epoch, batch_size=batch_size,validation_data=(input_dev_array,devY))\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history_object.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    \n",
    "    \n",
    "    record[key] = {'parameters':[learning_rate,dropout,n_dense,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,batch_size,epoch],'val_acc':accuracy}\n",
    "    \n",
    "    model.save('models/'+str(key)+'.h5')\n",
    "    \n",
    "    with open('models/record.pickle', 'wb') as handle:\n",
    "        pickle.dump(record, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    key+=1\n",
    "    \n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=0------------------\n",
      "learning rate ==> 0.0001\n",
      "dropout==> 0.5\n",
      "n_dense==> 100\n",
      "n_filters==> 100\n",
      "filter_size_c1 2\n",
      "filter_size_c2 4\n",
      "filter_size_c3 6\n",
      "em_c1==> embedding_matrix_word2vec\n",
      "em_c2==> embedding_matrix_godin\n",
      "em_c3==> embedding_matrix_word2vec\n",
      "batch_size==> 50\n",
      "epocs==> 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 16, 400)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 16, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 15, 100)      60100       input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 13, 100)      160100      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 11, 100)      180100      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 15, 100)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 13, 100)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 11, 100)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 7, 100)       0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 6, 100)       0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5, 100)       0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 700)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 600)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 500)          0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1800)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          180100      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            202         dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 580,602\n",
      "Trainable params: 580,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 100 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 0.6844 - acc: 0.5600 - val_loss: 0.6939 - val_acc: 0.4000\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6798 - acc: 0.5900 - val_loss: 0.6920 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6689 - acc: 0.5700 - val_loss: 0.6906 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6585 - acc: 0.6300 - val_loss: 0.6891 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6479 - acc: 0.7000 - val_loss: 0.6873 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6435 - acc: 0.7300 - val_loss: 0.6860 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6338 - acc: 0.7700 - val_loss: 0.6853 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6252 - acc: 0.7500 - val_loss: 0.6843 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6039 - acc: 0.8700 - val_loss: 0.6832 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6227 - acc: 0.7800 - val_loss: 0.6820 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI',\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('models/record.pickle', 'wb') as handle:\n",
    "    pickle.dump(record, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
