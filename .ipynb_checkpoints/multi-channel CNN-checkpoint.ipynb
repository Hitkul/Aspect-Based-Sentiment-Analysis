{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi channel CNN for sentiment analysis\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import remove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import fasttext\n",
    "import csv\n",
    "import codecs\n",
    "import word2vecReader as godin_embedding\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from gensim.models import KeyedVectors\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "def load_data_from_file(filename):\n",
    "    print(\"loading file = \",filename)\n",
    "    sentences = []\n",
    "    label = []\n",
    "    with codecs.open(filename, \"r\",encoding='utf-8', errors='ignore') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            try:\n",
    "                sentences.append(row[0])\n",
    "                label.append(row[1])\n",
    "            except:\n",
    "                print(row)\n",
    "    return sentences,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file =  dataset/final_train.csv\n",
      "loading file =  dataset/final_dev.csv\n"
     ]
    }
   ],
   "source": [
    "# sentences,score = load_data_from_xml('dataset/financial_posts_ABSA_train.xml')\n",
    "trainX,trainY = load_data_from_file('dataset/final_train.csv')\n",
    "devX,devY = load_data_from_file('dataset/final_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only using 1%data for testing code\n",
    "# trainX = trainX[:int(len(trainX)*0.001)]\n",
    "# trainY = trainY[:int(len(trainY)*0.001)]\n",
    "# devX = devX[:int(len(devX)*0.001)]\n",
    "# devY = devY[:int(len(devY)*0.001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", sentence)\n",
    "    #removing stock names to see if it helps\n",
    "    sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train set\n",
      "cleaning dev set\n"
     ]
    }
   ],
   "source": [
    "# extract sentences out of df and cleaning it\n",
    "print('cleaning train set')\n",
    "trainX = [clean_sentence(x) for x in trainX]\n",
    "print('cleaning dev set')\n",
    "devX = [clean_sentence(x) for x in devX]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copying sentences for fastext traning \n",
    "tranLines = list(trainX)\n",
    "devLines = list(devX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting real number scores to lables\n",
    "#0-->-ve sentiment 1-->+ve sentiment\n",
    "# labels = [1 if x >= 0 else 0 for x in score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def shuffle_data(sentences,labels,score):\n",
    "#     numbers = [i for i in range(len(sentences))]\n",
    "#     shuffle(numbers)\n",
    "#     temp_text = sentences\n",
    "#     temp_lables = labels\n",
    "#     temp_score = score\n",
    "#     for i in numbers:\n",
    "#         sentences[i] = temp_text[i]\n",
    "#         labels[i]=temp_lables[i]\n",
    "#         score[i] = temp_score[i]\n",
    "#     print(len(sentences))\n",
    "#     print(len(labels))\n",
    "#     print(len(score))\n",
    "#     return sentences,labels,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences,labels,score = shuffle_data(sentences,labels,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doing train and train split\n",
    "# test_train_split_ratio =0.9\n",
    "# trainX,testX = sentences[:int(test_train_split_ratio*len(sentences))],sentences[int(test_train_split_ratio*len(sentences)):]\n",
    "# trainY,testY = labels[:int(test_train_split_ratio*len(labels))],labels[int(test_train_split_ratio*len(labels)):]\n",
    "# score_trainY,score_testY = score[:int(test_train_split_ratio*len(score))],score[int(test_train_split_ratio*len(score)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569 1569\n",
      "16 16\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX),len(trainY))\n",
    "print(len(devX),len(devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# devY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting output matrix [-ve,+ve]\n",
    "devY = to_categorical(devY,2)\n",
    "trainY = to_categorical(trainY,2)\n",
    "# devY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# devY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 23\n",
      "Vocabulary size: 4263\n",
      "(1569, 23) (16, 23)\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainX)\n",
    "# calculate max document length\n",
    "lengths = [len(s.split()) for s in trainX]\n",
    "max_length = max(lengths)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainX, max_length)\n",
    "devX = encode_text(tokenizer, devX, max_length)\n",
    "print(trainX.shape,devX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04267465, 0.08063846, 0.08063846, 0.10391554, 0.05071078,\n",
       "        0.03934935, 0.03159032, 0.00304819, 0.00110843, 0.00110843]),\n",
       " array([ 0. ,  2.3,  4.6,  6.9,  9.2, 11.5, 13.8, 16.1, 18.4, 20.7, 23. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAJCCAYAAADUa5GyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f241086ff60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(12,10))\n",
    "plt.hist(lengths, normed=True,edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "considring only few sentences have len >20, we can also take max_len = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index_glove = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "embedding_matrix_glove = get_GloVe_embedding_matrix(embeddings_index_glove)\n",
    "# embedding_matrix[100]\n",
    "# e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')\n",
    "embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "#                 sentence = sentence.encode('UTF-8')\n",
    "#                 print(sentence)\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning new model\n",
      "model trained\n"
     ]
    }
   ],
   "source": [
    "#need to fix this\n",
    "fast_text_model = load_fast_text_model(tranLines+devLines)\n",
    "embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading the model, this can take some time...\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model, this can take some time...\n"
     ]
    }
   ],
   "source": [
    "godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dropout = Real(low=0.4, high=0.9,name = 'dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para_n_dense = Integer(low=100, high=400, name='n_dense')\n",
    "para_n_dense = Categorical(categories=[100,200,300,400], name='n_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para_n_filters = Integer(low=100,high=400,name='n_filters')\n",
    "para_n_filters = Categorical(categories=[100,200,300,400],name='n_filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c1 = Integer(low=1,high=6,name = 'filter_size_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c2 = Integer(low=1,high=6,name = 'filter_size_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c3 = Integer(low=1,high=6,name = 'filter_size_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'embedding_matrix_fast_text',\n",
    "para_em_c1 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_c2 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_c3 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_trainable_flag = Categorical(categories=[True,False],name='em_trainable_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_free_em_dim = Categorical(categories=[100,300,400],name='free_em_dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_batch_size = Categorical(categories=[50,100,150],name='batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ,50,100,200,300,400,500\n",
    "para_epoch = Categorical(categories=[10,50,100,200,300,400,500],name='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size_c1,para_filter_size_c2,para_filter_size_c3,para_em_c1,para_em_c2,para_em_c3,para_em_trainable_flag,para_free_em_dim,para_batch_size,para_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_parameters = [1e-4,0.5,100,100,2,4,6,'embedding_matrix_word2vec','embedding_matrix_glove','free',False,100,50,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    if em_c1 == 'free':\n",
    "        embedding1 = Embedding(vocab_size, free_em_dim)(inputs1)\n",
    "    else:\n",
    "        embedding1 = Embedding(vocab_size, len(eval(em_c1)[0]), weights = [eval(em_c1)],input_length=length,trainable = em_trainable_flag)(inputs1)\n",
    "\n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size_c1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "#     embedding2 = Embedding(vocab_size, 400, weights = [embedding_matrix_godin],input_length=length,trainable = em_trainable_flag)(inputs2)\n",
    "    if em_c2 == 'free':\n",
    "        embedding2 = Embedding(vocab_size, free_em_dim)(inputs2)\n",
    "    else:\n",
    "        embedding2 = Embedding(vocab_size, len(eval(em_c2)[0]), weights = [eval(em_c2)],input_length=length,trainable = em_trainable_flag)(inputs2)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=filter_size_c2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "#     embedding3 = Embedding(vocab_size, 400)(inputs3)\n",
    "    if em_c3 == 'free':\n",
    "        embedding3 = Embedding(vocab_size, free_em_dim)(inputs3)\n",
    "    else:\n",
    "        embedding3 = Embedding(vocab_size, len(eval(em_c3)[0]), weights = [eval(em_c3)],input_length=length,trainable = em_trainable_flag)(inputs3)\n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=filter_size_c3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(n_dense, activation='relu')(merged)\n",
    "    outputs = Dense(2, activation='softmax')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "#     print(model.summary())\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict to store performance of all models\n",
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,dropout,n_dense,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,em_trainable_flag,free_em_dim,batch_size,epoch):\n",
    "# n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag\n",
    "    # Print the hyper-parameters.\n",
    "    global key\n",
    "    global record\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    print('learning rate ==>',learning_rate)\n",
    "    print('dropout==>',dropout)\n",
    "    print('n_dense==>',n_dense)\n",
    "    print('n_filters==>',n_filters)\n",
    "    print('filter_size_c1',filter_size_c1)\n",
    "    print('filter_size_c2',filter_size_c2)\n",
    "    print('filter_size_c3',filter_size_c3)\n",
    "    print('em_c1==>',em_c1)\n",
    "    print('em_c2==>',em_c2)\n",
    "    print('em_c3==>',em_c3)\n",
    "    print('em_trainable_flag ==>',em_trainable_flag)\n",
    "    print('free_em_dim==>',free_em_dim)\n",
    "    print('batch_size==>',batch_size)\n",
    "    print('epocs==>',epoch)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = define_model(length = max_length,\n",
    "                         vocab_size=vocab_size,\n",
    "                         n_dense=n_dense,\n",
    "                         dropout=dropout,\n",
    "                         learning_rate=learning_rate,\n",
    "                         n_filters=n_filters,\n",
    "                         filter_size_c1=int(filter_size_c1),\n",
    "                         filter_size_c2=int(filter_size_c2),\n",
    "                         filter_size_c3=int(filter_size_c3),\n",
    "                         em_c1=em_c1,\n",
    "                         em_c2=em_c2,\n",
    "                         em_c3=em_c3,\n",
    "                         free_em_dim=free_em_dim,\n",
    "                         em_trainable_flag=em_trainable_flag)\n",
    "\n",
    "    \n",
    "    # Use Keras to train the model.\n",
    "    history_object = model.fit([trainX,trainX,trainX], trainY,epochs=epoch, batch_size=batch_size,validation_data=([devX,devX,devX],devY))\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history_object.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    \n",
    "    \n",
    "    record[key] = {'parameters':[learning_rate,dropout,n_dense,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,em_trainable_flag,free_em_dim,batch_size,epoch],'val_acc':accuracy}\n",
    "    \n",
    "    model.save('models/'+str(key)+'.h5')\n",
    "    \n",
    "    key+=1\n",
    "    \n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=0------------------\n",
      "learning rate ==> 0.0001\n",
      "dropout==> 0.5\n",
      "n_dense==> 100\n",
      "n_filters==> 100\n",
      "filter_size_c1 2\n",
      "filter_size_c2 4\n",
      "filter_size_c3 6\n",
      "em_c1==> embedding_matrix_word2vec\n",
      "em_c2==> embedding_matrix_glove\n",
      "em_c3==> free\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 100\n",
      "batch_size==> 50\n",
      "epocs==> 10\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6986 - acc: 0.5099 - val_loss: 0.7000 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6676 - acc: 0.6023 - val_loss: 0.6957 - val_acc: 0.4375\n",
      "Epoch 3/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6506 - acc: 0.6399 - val_loss: 0.6942 - val_acc: 0.4375\n",
      "Epoch 4/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6258 - acc: 0.6909 - val_loss: 0.6898 - val_acc: 0.4375\n",
      "Epoch 5/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6025 - acc: 0.7106 - val_loss: 0.6755 - val_acc: 0.6250\n",
      "Epoch 6/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.5706 - acc: 0.7368 - val_loss: 0.6791 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.5320 - acc: 0.7712 - val_loss: 0.6807 - val_acc: 0.5625\n",
      "Epoch 8/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.4944 - acc: 0.7967 - val_loss: 0.6608 - val_acc: 0.5625\n",
      "Epoch 9/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.4645 - acc: 0.8177 - val_loss: 0.6732 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.4278 - acc: 0.8407 - val_loss: 0.6630 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n",
      "-----------------------------combination no=1------------------\n",
      "learning rate ==> 0.0001303935287474031\n",
      "dropout==> 0.7881166818054242\n",
      "n_dense==> 200\n",
      "n_filters==> 400\n",
      "filter_size_c1 3\n",
      "filter_size_c2 6\n",
      "filter_size_c3 3\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_word2vec\n",
      "em_c3==> embedding_matrix_godin\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 150\n",
      "epocs==> 8\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/8\n",
      "1569/1569 [==============================] - 12s 8ms/step - loss: 0.7032 - acc: 0.5300 - val_loss: 0.6994 - val_acc: 0.4375\n",
      "Epoch 2/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.6673 - acc: 0.6017 - val_loss: 0.6875 - val_acc: 0.5625\n",
      "Epoch 3/8\n",
      "1569/1569 [==============================] - 12s 8ms/step - loss: 0.6398 - acc: 0.6495 - val_loss: 0.6812 - val_acc: 0.4375\n",
      "Epoch 4/8\n",
      "1569/1569 [==============================] - 13s 8ms/step - loss: 0.6032 - acc: 0.7030 - val_loss: 0.6840 - val_acc: 0.5000\n",
      "Epoch 5/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.5687 - acc: 0.7323 - val_loss: 0.6715 - val_acc: 0.5625\n",
      "Epoch 6/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.5268 - acc: 0.7674 - val_loss: 0.6542 - val_acc: 0.5625\n",
      "Epoch 7/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.4949 - acc: 0.7929 - val_loss: 0.6548 - val_acc: 0.5625\n",
      "Epoch 8/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.4420 - acc: 0.8177 - val_loss: 0.6796 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n",
      "-----------------------------combination no=2------------------\n",
      "learning rate ==> 0.00543087312620765\n",
      "dropout==> 0.8596902940034467\n",
      "n_dense==> 100\n",
      "n_filters==> 400\n",
      "filter_size_c1 4\n",
      "filter_size_c2 5\n",
      "filter_size_c3 4\n",
      "em_c1==> free\n",
      "em_c2==> embedding_matrix_word2vec\n",
      "em_c3==> embedding_matrix_word2vec\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 100\n",
      "epocs==> 10\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 1.0445 - acc: 0.5112 - val_loss: 0.6849 - val_acc: 0.6250\n",
      "Epoch 2/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.6705 - acc: 0.5730 - val_loss: 0.6573 - val_acc: 0.5625\n",
      "Epoch 3/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.3598 - acc: 0.8477 - val_loss: 0.7771 - val_acc: 0.6250\n",
      "Epoch 4/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.1308 - acc: 0.9509 - val_loss: 0.7248 - val_acc: 0.6250\n",
      "Epoch 5/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0659 - acc: 0.9796 - val_loss: 0.8402 - val_acc: 0.6875\n",
      "Epoch 6/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0385 - acc: 0.9892 - val_loss: 1.2451 - val_acc: 0.6875\n",
      "Epoch 7/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0255 - acc: 0.9924 - val_loss: 1.2050 - val_acc: 0.6875\n",
      "Epoch 8/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0181 - acc: 0.9936 - val_loss: 1.4865 - val_acc: 0.6875\n",
      "Epoch 9/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0352 - acc: 0.9885 - val_loss: 1.5816 - val_acc: 0.6250\n",
      "Epoch 10/10\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0240 - acc: 0.9936 - val_loss: 1.6774 - val_acc: 0.6875\n",
      "Accuracy: 68.75%\n",
      "-----------------------------combination no=3------------------\n",
      "learning rate ==> 0.0008784302296675664\n",
      "dropout==> 0.4109988879439824\n",
      "n_dense==> 400\n",
      "n_filters==> 300\n",
      "filter_size_c1 4\n",
      "filter_size_c2 4\n",
      "filter_size_c3 5\n",
      "em_c1==> embedding_matrix_godin\n",
      "em_c2==> embedding_matrix_fast_text\n",
      "em_c3==> free\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 400\n",
      "batch_size==> 50\n",
      "epocs==> 5\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.7032 - acc: 0.5284 - val_loss: 0.6948 - val_acc: 0.3750\n",
      "Epoch 2/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.5136 - acc: 0.7884 - val_loss: 0.6335 - val_acc: 0.7500\n",
      "Epoch 3/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.1552 - acc: 0.9407 - val_loss: 0.7852 - val_acc: 0.6875\n",
      "Epoch 4/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0425 - acc: 0.9834 - val_loss: 0.8803 - val_acc: 0.5625\n",
      "Epoch 5/5\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.0130 - acc: 0.9975 - val_loss: 1.1222 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n",
      "-----------------------------combination no=4------------------\n",
      "learning rate ==> 0.00944910795479221\n",
      "dropout==> 0.5259951916281641\n",
      "n_dense==> 200\n",
      "n_filters==> 100\n",
      "filter_size_c1 2\n",
      "filter_size_c2 4\n",
      "filter_size_c3 6\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_glove\n",
      "em_c3==> embedding_matrix_fast_text\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 150\n",
      "epocs==> 4\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/4\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 1.5284 - acc: 0.5003 - val_loss: 0.6886 - val_acc: 0.6250\n",
      "Epoch 2/4\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.6424 - acc: 0.6201 - val_loss: 0.6769 - val_acc: 0.6250\n",
      "Epoch 3/4\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.4075 - acc: 0.8203 - val_loss: 0.7871 - val_acc: 0.6875\n",
      "Epoch 4/4\n",
      "1569/1569 [==============================] - 2s 1ms/step - loss: 0.1520 - acc: 0.9446 - val_loss: 0.7805 - val_acc: 0.6250\n",
      "Accuracy: 62.50%\n",
      "-----------------------------combination no=5------------------\n",
      "learning rate ==> 0.0022466095443656854\n",
      "dropout==> 0.429143995306156\n",
      "n_dense==> 300\n",
      "n_filters==> 300\n",
      "filter_size_c1 6\n",
      "filter_size_c2 3\n",
      "filter_size_c3 4\n",
      "em_c1==> embedding_matrix_godin\n",
      "em_c2==> embedding_matrix_godin\n",
      "em_c3==> embedding_matrix_fast_text\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 100\n",
      "batch_size==> 100\n",
      "epocs==> 3\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/3\n",
      "1569/1569 [==============================] - 6s 4ms/step - loss: 0.7548 - acc: 0.4990 - val_loss: 0.6917 - val_acc: 0.4375\n",
      "Epoch 2/3\n",
      "1569/1569 [==============================] - 5s 3ms/step - loss: 0.6679 - acc: 0.6074 - val_loss: 0.6769 - val_acc: 0.4375\n",
      "Epoch 3/3\n",
      "1569/1569 [==============================] - 5s 3ms/step - loss: 0.5217 - acc: 0.7476 - val_loss: 0.7198 - val_acc: 0.6875\n",
      "Accuracy: 68.75%\n",
      "-----------------------------combination no=6------------------\n",
      "learning rate ==> 0.005365325804556098\n",
      "dropout==> 0.6152436648610217\n",
      "n_dense==> 400\n",
      "n_filters==> 300\n",
      "filter_size_c1 6\n",
      "filter_size_c2 3\n",
      "filter_size_c3 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em_c1==> free\n",
      "em_c2==> embedding_matrix_glove\n",
      "em_c3==> embedding_matrix_glove\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 100\n",
      "epocs==> 3\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/3\n",
      "1569/1569 [==============================] - 7s 4ms/step - loss: 4.3009 - acc: 0.5022 - val_loss: 0.7320 - val_acc: 0.3750\n",
      "Epoch 2/3\n",
      "1569/1569 [==============================] - 8s 5ms/step - loss: 0.7271 - acc: 0.5532 - val_loss: 0.6406 - val_acc: 0.7500\n",
      "Epoch 3/3\n",
      "1569/1569 [==============================] - 8s 5ms/step - loss: 0.5206 - acc: 0.7234 - val_loss: 0.6321 - val_acc: 0.5625\n",
      "Accuracy: 56.25%\n",
      "-----------------------------combination no=7------------------\n",
      "learning rate ==> 0.0006209448862127463\n",
      "dropout==> 0.6674276408699704\n",
      "n_dense==> 400\n",
      "n_filters==> 200\n",
      "filter_size_c1 2\n",
      "filter_size_c2 2\n",
      "filter_size_c3 3\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_fast_text\n",
      "em_c3==> embedding_matrix_godin\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 300\n",
      "batch_size==> 150\n",
      "epocs==> 8\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.7602 - acc: 0.5029 - val_loss: 0.7140 - val_acc: 0.3125\n",
      "Epoch 2/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.6561 - acc: 0.6017 - val_loss: 0.7042 - val_acc: 0.4375\n",
      "Epoch 3/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.6068 - acc: 0.6960 - val_loss: 0.6820 - val_acc: 0.6250\n",
      "Epoch 4/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.5380 - acc: 0.7419 - val_loss: 0.6825 - val_acc: 0.6250\n",
      "Epoch 5/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.4453 - acc: 0.8107 - val_loss: 0.7091 - val_acc: 0.6875\n",
      "Epoch 6/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.3717 - acc: 0.8438 - val_loss: 0.7667 - val_acc: 0.6875\n",
      "Epoch 7/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.3290 - acc: 0.8636 - val_loss: 0.7072 - val_acc: 0.6875\n",
      "Epoch 8/8\n",
      "1569/1569 [==============================] - 3s 2ms/step - loss: 0.2569 - acc: 0.9063 - val_loss: 0.7001 - val_acc: 0.6875\n",
      "Accuracy: 68.75%\n",
      "-----------------------------combination no=8------------------\n",
      "learning rate ==> 0.001158798655778977\n",
      "dropout==> 0.8948764310021187\n",
      "n_dense==> 100\n",
      "n_filters==> 400\n",
      "filter_size_c1 5\n",
      "filter_size_c2 3\n",
      "filter_size_c3 3\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_fast_text\n",
      "em_c3==> embedding_matrix_word2vec\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 300\n",
      "batch_size==> 50\n",
      "epocs==> 8\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/8\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.7251 - acc: 0.5583 - val_loss: 0.6836 - val_acc: 0.5000\n",
      "Epoch 2/8\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.6128 - acc: 0.6871 - val_loss: 0.6931 - val_acc: 0.6875\n",
      "Epoch 3/8\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.5050 - acc: 0.7584 - val_loss: 0.6684 - val_acc: 0.5625\n",
      "Epoch 4/8\n",
      "1569/1569 [==============================] - 9s 6ms/step - loss: 0.3962 - acc: 0.8215 - val_loss: 0.5773 - val_acc: 0.7500\n",
      "Epoch 5/8\n",
      "1569/1569 [==============================] - 10s 7ms/step - loss: 0.2553 - acc: 0.8936 - val_loss: 0.6784 - val_acc: 0.6875\n",
      "Epoch 6/8\n",
      "1569/1569 [==============================] - 11s 7ms/step - loss: 0.1942 - acc: 0.9222 - val_loss: 0.7096 - val_acc: 0.6875\n",
      "Epoch 7/8\n",
      "1569/1569 [==============================] - 10s 6ms/step - loss: 0.1169 - acc: 0.9662 - val_loss: 0.8994 - val_acc: 0.6875\n",
      "Epoch 8/8\n",
      "1569/1569 [==============================] - 10s 6ms/step - loss: 0.0916 - acc: 0.9611 - val_loss: 0.9682 - val_acc: 0.6875\n",
      "Accuracy: 68.75%\n",
      "-----------------------------combination no=9------------------\n",
      "learning rate ==> 0.000472944544348041\n",
      "dropout==> 0.6115958972311217\n",
      "n_dense==> 300\n",
      "n_filters==> 400\n",
      "filter_size_c1 6\n",
      "filter_size_c2 2\n",
      "filter_size_c3 3\n",
      "em_c1==> free\n",
      "em_c2==> embedding_matrix_godin\n",
      "em_c3==> free\n",
      "em_trainable_flag ==> True\n",
      "free_em_dim==> 100\n",
      "batch_size==> 100\n",
      "epocs==> 5\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "1569/1569 [==============================] - 8s 5ms/step - loss: 0.6997 - acc: 0.5118 - val_loss: 0.6982 - val_acc: 0.3750\n",
      "Epoch 2/5\n",
      "1569/1569 [==============================] - 6s 4ms/step - loss: 0.6193 - acc: 0.7266 - val_loss: 0.6425 - val_acc: 0.6250\n",
      "Epoch 3/5\n",
      "1569/1569 [==============================] - 6s 4ms/step - loss: 0.4125 - acc: 0.8693 - val_loss: 0.5552 - val_acc: 0.6250\n",
      "Epoch 4/5\n",
      "1569/1569 [==============================] - 7s 4ms/step - loss: 0.1897 - acc: 0.9433 - val_loss: 0.5841 - val_acc: 0.8125\n",
      "Epoch 5/5\n",
      "1569/1569 [==============================] - 8s 5ms/step - loss: 0.0806 - acc: 0.9726 - val_loss: 0.7572 - val_acc: 0.6250\n",
      "Accuracy: 62.50%\n",
      "-----------------------------combination no=10------------------\n",
      "learning rate ==> 0.0002304493811652546\n",
      "dropout==> 0.8378794135947594\n",
      "n_dense==> 400\n",
      "n_filters==> 300\n",
      "filter_size_c1 2\n",
      "filter_size_c2 3\n",
      "filter_size_c3 2\n",
      "em_c1==> embedding_matrix_glove\n",
      "em_c2==> embedding_matrix_godin\n",
      "em_c3==> embedding_matrix_fast_text\n",
      "em_trainable_flag ==> False\n",
      "free_em_dim==> 300\n",
      "batch_size==> 100\n",
      "epocs==> 3\n",
      "Train on 1569 samples, validate on 16 samples\n",
      "Epoch 1/3\n",
      "1569/1569 [==============================] - 6s 4ms/step - loss: 0.7113 - acc: 0.5249 - val_loss: 0.6973 - val_acc: 0.5000\n",
      "Epoch 2/3\n",
      "1569/1569 [==============================] - 4s 3ms/step - loss: 0.6693 - acc: 0.6080 - val_loss: 0.6950 - val_acc: 0.4375\n",
      "Epoch 3/3\n",
      "1569/1569 [==============================] - 5s 3ms/step - loss: 0.6393 - acc: 0.6495 - val_loss: 0.6974 - val_acc: 0.5000\n",
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI',\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.6875,\n",
       "  [0.0006209448862127463,\n",
       "   0.6674276408699704,\n",
       "   400,\n",
       "   200,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'embedding_matrix_godin',\n",
       "   False,\n",
       "   300,\n",
       "   150,\n",
       "   8]),\n",
       " (-0.6875,\n",
       "  [0.001158798655778977,\n",
       "   0.8948764310021187,\n",
       "   100,\n",
       "   400,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'embedding_matrix_word2vec',\n",
       "   True,\n",
       "   300,\n",
       "   50,\n",
       "   8]),\n",
       " (-0.6875,\n",
       "  [0.0022466095443656854,\n",
       "   0.429143995306156,\n",
       "   300,\n",
       "   300,\n",
       "   6,\n",
       "   3,\n",
       "   4,\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   False,\n",
       "   100,\n",
       "   100,\n",
       "   3]),\n",
       " (-0.6875,\n",
       "  [0.00543087312620765,\n",
       "   0.8596902940034467,\n",
       "   100,\n",
       "   400,\n",
       "   4,\n",
       "   5,\n",
       "   4,\n",
       "   'free',\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_word2vec',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   10]),\n",
       " (-0.625,\n",
       "  [0.000472944544348041,\n",
       "   0.6115958972311217,\n",
       "   300,\n",
       "   400,\n",
       "   6,\n",
       "   2,\n",
       "   3,\n",
       "   'free',\n",
       "   'embedding_matrix_godin',\n",
       "   'free',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   5]),\n",
       " (-0.625,\n",
       "  [0.00944910795479221,\n",
       "   0.5259951916281641,\n",
       "   200,\n",
       "   100,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   True,\n",
       "   100,\n",
       "   150,\n",
       "   4]),\n",
       " (-0.5625,\n",
       "  [0.005365325804556098,\n",
       "   0.6152436648610217,\n",
       "   400,\n",
       "   300,\n",
       "   6,\n",
       "   3,\n",
       "   3,\n",
       "   'free',\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_glove',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   3]),\n",
       " (-0.5,\n",
       "  [0.0001,\n",
       "   0.5,\n",
       "   100,\n",
       "   100,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_glove',\n",
       "   'free',\n",
       "   False,\n",
       "   100,\n",
       "   50,\n",
       "   10]),\n",
       " (-0.5,\n",
       "  [0.0001303935287474031,\n",
       "   0.7881166818054242,\n",
       "   200,\n",
       "   400,\n",
       "   3,\n",
       "   6,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_godin',\n",
       "   True,\n",
       "   100,\n",
       "   150,\n",
       "   8]),\n",
       " (-0.5,\n",
       "  [0.0002304493811652546,\n",
       "   0.8378794135947594,\n",
       "   400,\n",
       "   300,\n",
       "   2,\n",
       "   3,\n",
       "   2,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   False,\n",
       "   300,\n",
       "   100,\n",
       "   3]),\n",
       " (-0.5,\n",
       "  [0.0008784302296675664,\n",
       "   0.4109988879439824,\n",
       "   400,\n",
       "   300,\n",
       "   4,\n",
       "   4,\n",
       "   5,\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'free',\n",
       "   False,\n",
       "   400,\n",
       "   50,\n",
       "   5])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'parameters': [0.0001,\n",
       "   0.5,\n",
       "   100,\n",
       "   100,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_glove',\n",
       "   'free',\n",
       "   False,\n",
       "   100,\n",
       "   50,\n",
       "   10],\n",
       "  'val_acc': 0.5},\n",
       " 1: {'parameters': [0.0001303935287474031,\n",
       "   0.7881166818054242,\n",
       "   200,\n",
       "   400,\n",
       "   3,\n",
       "   6,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_godin',\n",
       "   True,\n",
       "   100,\n",
       "   150,\n",
       "   8],\n",
       "  'val_acc': 0.5},\n",
       " 2: {'parameters': [0.00543087312620765,\n",
       "   0.8596902940034467,\n",
       "   100,\n",
       "   400,\n",
       "   4,\n",
       "   5,\n",
       "   4,\n",
       "   'free',\n",
       "   'embedding_matrix_word2vec',\n",
       "   'embedding_matrix_word2vec',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   10],\n",
       "  'val_acc': 0.6875},\n",
       " 3: {'parameters': [0.0008784302296675664,\n",
       "   0.4109988879439824,\n",
       "   400,\n",
       "   300,\n",
       "   4,\n",
       "   4,\n",
       "   5,\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'free',\n",
       "   False,\n",
       "   400,\n",
       "   50,\n",
       "   5],\n",
       "  'val_acc': 0.5},\n",
       " 4: {'parameters': [0.00944910795479221,\n",
       "   0.5259951916281641,\n",
       "   200,\n",
       "   100,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   True,\n",
       "   100,\n",
       "   150,\n",
       "   4],\n",
       "  'val_acc': 0.625},\n",
       " 5: {'parameters': [0.0022466095443656854,\n",
       "   0.429143995306156,\n",
       "   300,\n",
       "   300,\n",
       "   6,\n",
       "   3,\n",
       "   4,\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   False,\n",
       "   100,\n",
       "   100,\n",
       "   3],\n",
       "  'val_acc': 0.6875},\n",
       " 6: {'parameters': [0.005365325804556098,\n",
       "   0.6152436648610217,\n",
       "   400,\n",
       "   300,\n",
       "   6,\n",
       "   3,\n",
       "   3,\n",
       "   'free',\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_glove',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   3],\n",
       "  'val_acc': 0.5625},\n",
       " 7: {'parameters': [0.0006209448862127463,\n",
       "   0.6674276408699704,\n",
       "   400,\n",
       "   200,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'embedding_matrix_godin',\n",
       "   False,\n",
       "   300,\n",
       "   150,\n",
       "   8],\n",
       "  'val_acc': 0.6875},\n",
       " 8: {'parameters': [0.001158798655778977,\n",
       "   0.8948764310021187,\n",
       "   100,\n",
       "   400,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_fast_text',\n",
       "   'embedding_matrix_word2vec',\n",
       "   True,\n",
       "   300,\n",
       "   50,\n",
       "   8],\n",
       "  'val_acc': 0.6875},\n",
       " 9: {'parameters': [0.000472944544348041,\n",
       "   0.6115958972311217,\n",
       "   300,\n",
       "   400,\n",
       "   6,\n",
       "   2,\n",
       "   3,\n",
       "   'free',\n",
       "   'embedding_matrix_godin',\n",
       "   'free',\n",
       "   True,\n",
       "   100,\n",
       "   100,\n",
       "   5],\n",
       "  'val_acc': 0.625},\n",
       " 10: {'parameters': [0.0002304493811652546,\n",
       "   0.8378794135947594,\n",
       "   400,\n",
       "   300,\n",
       "   2,\n",
       "   3,\n",
       "   2,\n",
       "   'embedding_matrix_glove',\n",
       "   'embedding_matrix_godin',\n",
       "   'embedding_matrix_fast_text',\n",
       "   False,\n",
       "   300,\n",
       "   100,\n",
       "   3],\n",
       "  'val_acc': 0.5}}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record\n",
    "# fit model\n",
    "# history_object = model.fit([trainX,trainX,trainX], trainY,epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/record.pickle', 'wb') as handle:\n",
    "    pickle.dump(record, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(12,10))\n",
    "# plt.plot(history_object.history['acc'])\n",
    "# # plt.plot(history_object.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# # plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(12,10))\n",
    "# plt.plot(history_object.history['loss'])\n",
    "# # plt.plot(history_object.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# # plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "# encode data\n",
    "# trainX = encode_text(tokenizer, trainLines, length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # evaluate model on training dataset\n",
    "# loss, acc = model.evaluate([trainX,trainX,trainX], trainY, verbose=0)\n",
    "# print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# # evaluate model on test dataset dataset\n",
    "# loss, acc = model.evaluate([testX,testX,testX], testY, verbose=0)\n",
    "# print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.predict([testX,testX,testX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_predicted_prob(data):\n",
    "#     return [list(x) for x in model.predict(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicted probabilities\n",
    "# train_predicted_prob = get_predicted_prob([trainX,trainX,trainX])\n",
    "# test_predicted_prob = get_predicted_prob([testX,testX,testX])\n",
    "# predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicting lables\n",
    "#if prob>0.5 -->1 else 0\n",
    "# def get_predicted_lables(prob):\n",
    "#     predicted_lables = []\n",
    "#     for x in prob:\n",
    "#         if x>0.5:\n",
    "#             predicted_lables.append(1)\n",
    "#         else:\n",
    "#             predicted_lables.append(0)\n",
    "#     return predicted_lables\n",
    "# def get_predicted_lables(prob):\n",
    "#     predicted_lables = []\n",
    "#     for x in prob:\n",
    "#         if x[0]>x[1]:\n",
    "#             predicted_lables.append(0)\n",
    "#         else:\n",
    "#             predicted_lables.append(1)\n",
    "#     return predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_lables = get_predicted_lables(train_predicted_prob)\n",
    "# test_predicted_lables = get_predicted_lables(test_predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # def get_score_from_prob(prob):\n",
    "# #     m = interp1d([0,1],[-1,1])\n",
    "# #     return [float(m(x)) for x in prob]\n",
    "# def get_score_from_prob(prob):\n",
    "#     intensity_score = []\n",
    "#     for x in prob:\n",
    "#         if x[0]>x[1]:\n",
    "#             intensity_score.append(-1*x[0])\n",
    "#         else:\n",
    "#             intensity_score.append(x[1])\n",
    "#     return intensity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_score = get_score_from_prob(train_predicted_prob)\n",
    "# test_predicted_score = get_score_from_prob(test_predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_rmse = sqrt(mean_squared_error(score_trainY, train_predicted_score))\n",
    "# test_rmse = sqrt(mean_squared_error(score_testY, test_predicted_score))\n",
    "# print(\"(train rmse,test rmse)==\"+str((train_rmse,test_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# foo = [x for x in zip(score_testY,test_predicted_score)]\n",
    "# foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
