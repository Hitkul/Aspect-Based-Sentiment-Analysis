{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#multi channel CNN for sentiment analysis\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import remove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import fasttext\n",
    "import word2vecReader as godin_embedding\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from gensim.models import KeyedVectors\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "def load_data_from_xml(filename):\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    sentences = list()\n",
    "    score= list()\n",
    "    for row in root:\n",
    "        score.append(float(row[1].text))\n",
    "        sentences.append(row[2].text)\n",
    "    return sentences,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences,score = load_data_from_xml('dataset/financial_posts_ABSA_train.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", sentence)\n",
    "    #removing stock names to see if it helps\n",
    "    sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract sentences out of df and cleaning it\n",
    "sentences = [clean_sentence(x) for x in sentences]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting real number scores to lables\n",
    "#0-->-ve sentiment 1-->+ve sentiment\n",
    "labels = [1 if x >= 0 else 0 for x in score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data(sentences,labels,score):\n",
    "    numbers = [i for i in range(len(sentences))]\n",
    "    shuffle(numbers)\n",
    "    temp_text = sentences\n",
    "    temp_lables = labels\n",
    "    temp_score = score\n",
    "    for i in numbers:\n",
    "        sentences[i] = temp_text[i]\n",
    "        labels[i]=temp_lables[i]\n",
    "        score[i] = temp_score[i]\n",
    "    print(len(sentences))\n",
    "    print(len(labels))\n",
    "    print(len(score))\n",
    "    return sentences,labels,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n",
      "675\n",
      "675\n"
     ]
    }
   ],
   "source": [
    "sentences,labels,score = shuffle_data(sentences,labels,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doing train and train split\n",
    "test_train_split_ratio =0.9\n",
    "trainX,testX = sentences[:int(test_train_split_ratio*len(sentences))],sentences[int(test_train_split_ratio*len(sentences)):]\n",
    "trainY,testY = labels[:int(test_train_split_ratio*len(labels))],labels[int(test_train_split_ratio*len(labels)):]\n",
    "score_trainY,score_testY = score[:int(test_train_split_ratio*len(score))],score[int(test_train_split_ratio*len(score)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607 607 607\n",
      "68 68 68\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX),len(trainY),len(score_trainY))\n",
    "print(len(testX),len(testY),len(score_testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting output matrix [-ve,+ve]\n",
    "testY = to_categorical(testY,2)\n",
    "trainY = to_categorical(trainY,2)\n",
    "# testY[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testY[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 21\n",
      "Vocabulary size: 2043\n",
      "(607, 21) (68, 21)\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainX)\n",
    "# calculate max document length\n",
    "lengths = [len(s.split()) for s in trainX]\n",
    "max_length = max(lengths)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % max_length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainX, max_length)\n",
    "testX = encode_text(tokenizer, testX, max_length)\n",
    "print(trainX.shape,testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04365733, 0.07495881, 0.09143328, 0.08649094, 0.07825371,\n",
       "        0.06507414, 0.03624382, 0.01812191, 0.00411862, 0.00164745]),\n",
       " array([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAJCCAYAAADUa5GyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu5s/OBbXHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc10f14c0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(12,10))\n",
    "plt.hist(lengths, normed=True,edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "considring only few sentences have len >15, we can also take max_len = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index_glove = load_GloVe_embedding('word_embeddings/glove.6B.300d.txt')\n",
    "embedding_matrix_glove = get_GloVe_embedding_matrix(embeddings_index_glove)\n",
    "# embedding_matrix[100]\n",
    "# e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model= load_google_word2vec('word_embeddings/GoogleNews-vectors-negative300.bin')\n",
    "embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast text word embedding\n",
    "def load_fast_text_model(sentences):\n",
    "    try:\n",
    "        m = fasttext.load_model('fast_text_model.bin')\n",
    "        print(\"trained model loaded\")\n",
    "        return m\n",
    "    except:\n",
    "        print(\"traning new model\")\n",
    "        with open('temp_file.txt','w') as temp_file:\n",
    "            for sentence in sentences:\n",
    "#                 sentence = sentence.encode('UTF-8')\n",
    "#                 print(sentence)\n",
    "                temp_file.write(sentence)\n",
    "        m = fasttext.cbow('temp_file.txt','fast_text_model')\n",
    "        remove('temp_file.txt')\n",
    "        print('model trained')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning new model\n",
      "model trained\n"
     ]
    }
   ],
   "source": [
    "fast_text_model = load_fast_text_model(trainLines+testLines)\n",
    "embedding_matrix_fast_text = get_fast_text_matrix(fast_text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading godin word embedding\n",
    "def load_godin_word_embedding(path):\n",
    "    print(\"Loading the model, this can take some time...\")\n",
    "    return godin_embedding.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_godin_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,400))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model, this can take some time...\n"
     ]
    }
   ],
   "source": [
    "godin_model = load_godin_word_embedding(\"word_embeddings/word2vec_twitter_model.bin\")\n",
    "embedding_matrix_godin = get_godin_embedding_matrix(godin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dropout = Real(low=0.4, high=0.9,name = 'dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para_n_dense = Integer(low=100, high=400, name='n_dense')\n",
    "para_n_dense = Categorical(categories=[100,200,300,400], name='n_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para_n_filters = Integer(low=100,high=400,name='n_filters')\n",
    "para_n_filters = Categorical(categories=[100,200,300,400],name='n_filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c1 = Integer(low=1,high=6,name = 'filter_size_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c2 = Integer(low=1,high=6,name = 'filter_size_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_filter_size_c3 = Integer(low=1,high=6,name = 'filter_size_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix_fast_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-114ec0de8c34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpara_em_c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix_fast_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix_godin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix_word2vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix_glove\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'free'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'em_c1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix_fast_text' is not defined"
     ]
    }
   ],
   "source": [
    "para_em_c1 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_c2 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_c3 = Categorical(categories=['embedding_matrix_fast_text','embedding_matrix_godin','embedding_matrix_word2vec','embedding_matrix_glove','free'],name='em_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_em_trainable_flag = Categorical(categories=[True,False],name='em_trainable_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_free_em_dim = Categorical(categories=[100,300,400],name='free_em_dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_batch_size = Categorical(categories=[50,100,150],name='batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_epoch = Categorical(categories=[10,50,100,200,300,400,500],name='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'para_em_c1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2fad85b323a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpara_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_dropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_n_dense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_n_filters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_filter_size_c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_filter_size_c2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_filter_size_c3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_em_c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_em_c2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_em_c3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_em_trainable_flag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'para_em_c1' is not defined"
     ]
    }
   ],
   "source": [
    "parameters = [para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size_c1,para_filter_size_c2,para_filter_size_c3,para_em_c1,para_em_c2,para_em_c3,para_em_trainable_flag,para_free_em_dim,para_batch_size,para_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix_word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7b8eeb70a10d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdefault_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix_word2vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix_glove\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'free'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix_word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "default_parameters = [1e-4,0.5,100,100,2,4,6,'embedding_matrix_word2vec','embedding_matrix_glove','free',False,100,50,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length,vocab_size,n_dense,dropout,learning_rate,n_filters,filter_size_c1,filter_size_c2,filter_size_c3,em_c1,em_c2,em_c3,free_em_dim,em_trainable_flag):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    if em_c1 == 'free':\n",
    "        embedding1 = Embedding(vocab_size, free_em_dim)(inputs1)\n",
    "    else:\n",
    "        embedding1 = Embedding(vocab_size, len(eval(em_c1)[0]), weights = [eval(em_c1)],input_length=length,trainable = em_trainable_flag)(inputs1)\n",
    "\n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size_c1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "#     embedding2 = Embedding(vocab_size, 400, weights = [embedding_matrix_godin],input_length=length,trainable = em_trainable_flag)(inputs2)\n",
    "    if em_c2 == 'free':\n",
    "        embedding2 = Embedding(vocab_size, free_em_dim)(inputs2)\n",
    "    else:\n",
    "        embedding2 = Embedding(vocab_size, len(eval(em_c2)[0]), weights = [eval(em_c2)],input_length=length,trainable = em_trainable_flag)(inputs2)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=filter_size_c2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "#     embedding3 = Embedding(vocab_size, 400)(inputs3)\n",
    "    if em_c3 == 'free':\n",
    "        embedding3 = Embedding(vocab_size, free_em_dim)(inputs3)\n",
    "    else:\n",
    "        embedding3 = Embedding(vocab_size, len(eval(em_c3)[0]), weights = [eval(em_c3)],input_length=length,trainable = em_trainable_flag)(inputs3)\n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=filter_size_c3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(n_dense, activation='relu')(merged)\n",
    "    outputs = Dense(2, activation='softmax')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # summarize\n",
    "#     print(model.summary())\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict to store performance of all models\n",
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(para_learning_rate,para_dropout,para_n_dense,para_n_filters,para_filter_size_c1,para_filter_size_c2,para_filter_size_c3,para_em_c1,para_em_c2,para_em_c3,para_em_trainable_flag,para_free_em_dim,para_batch_size,para_epoch):\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate ==>',para_learning_rate)\n",
    "    print('dropout==>',para_dropout)\n",
    "    print('n_dense==>',para_n_dense)\n",
    "    print('n_filters==>',para_n_filters)\n",
    "    print('filter_size_c1',para_filter_size_c1)\n",
    "    print('filter_size_c2',para_filter_size_c2)\n",
    "    print('filter_size_c3',para_filter_size_c3)\n",
    "    print('em_c1==>',para_em_c1)\n",
    "    print('em_c2==>',para_em_c2)\n",
    "    print('em_c3==>',para_em_c3)\n",
    "    print('em_trainable_flag ==>',para_em_trainable_flag)\n",
    "    print('free_em_dim==>',para_free_em_dim)\n",
    "    print('batch_size==>',para_batch_size)\n",
    "    print('epocs==>',para_epoch)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = define_model(length = max_length,\n",
    "                         vocab_size=vocab_size,\n",
    "                         n_dense=para_n_dense,\n",
    "                         dropout=para_dropout,\n",
    "                         learning_rate=para_learning_rate,\n",
    "                         n_filters=para_n_filters,\n",
    "                         filter_size_c1=para_filter_size_c1,\n",
    "                         filter_size_c2=para_filter_size_c2,\n",
    "                         filter_size_c3=para_filter_size_c3,\n",
    "                         em_c1=para_em_c1,\n",
    "                         em_c2 = para_em_c2,\n",
    "                         em_c3=para_em_c3,\n",
    "                         free_em_dim=para_free_em_dim,\n",
    "                         em_trainable_flag=para_em_trainable_flag)\n",
    "\n",
    "    \n",
    "    # Use Keras to train the model.\n",
    "    history_object = model.fit([trainX,trainX,trainX], trainY,epochs=para_epoch, batch_size=para_batch_size)\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history_object.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    \n",
    "    global key\n",
    "    global record\n",
    "    record[key] = {'parameters':dimensions,'val_acc':accuracy}\n",
    "    \n",
    "    model.save('models/'+str(key)+'.h5')\n",
    "    \n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI',\n",
    "                            n_calls=500,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sorted(zip(search_result.func_vals, search_result.x_iters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "# history_object = model.fit([trainX,trainX,trainX], trainY,epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(12,10))\n",
    "# plt.plot(history_object.history['acc'])\n",
    "# # plt.plot(history_object.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# # plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(12,10))\n",
    "# plt.plot(history_object.history['loss'])\n",
    "# # plt.plot(history_object.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# # plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "# encode data\n",
    "# trainX = encode_text(tokenizer, trainLines, length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model on training dataset\n",
    "# loss, acc = model.evaluate([trainX,trainX,trainX], trainY, verbose=0)\n",
    "# print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# # evaluate model on test dataset dataset\n",
    "# loss, acc = model.evaluate([testX,testX,testX], testY, verbose=0)\n",
    "# print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.predict([testX,testX,testX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_predicted_prob(data):\n",
    "#     return [list(x) for x in model.predict(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicted probabilities\n",
    "# train_predicted_prob = get_predicted_prob([trainX,trainX,trainX])\n",
    "# test_predicted_prob = get_predicted_prob([testX,testX,testX])\n",
    "# predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicting lables\n",
    "#if prob>0.5 -->1 else 0\n",
    "# def get_predicted_lables(prob):\n",
    "#     predicted_lables = []\n",
    "#     for x in prob:\n",
    "#         if x>0.5:\n",
    "#             predicted_lables.append(1)\n",
    "#         else:\n",
    "#             predicted_lables.append(0)\n",
    "#     return predicted_lables\n",
    "# def get_predicted_lables(prob):\n",
    "#     predicted_lables = []\n",
    "#     for x in prob:\n",
    "#         if x[0]>x[1]:\n",
    "#             predicted_lables.append(0)\n",
    "#         else:\n",
    "#             predicted_lables.append(1)\n",
    "#     return predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_lables = get_predicted_lables(train_predicted_prob)\n",
    "# test_predicted_lables = get_predicted_lables(test_predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # def get_score_from_prob(prob):\n",
    "# #     m = interp1d([0,1],[-1,1])\n",
    "# #     return [float(m(x)) for x in prob]\n",
    "# def get_score_from_prob(prob):\n",
    "#     intensity_score = []\n",
    "#     for x in prob:\n",
    "#         if x[0]>x[1]:\n",
    "#             intensity_score.append(-1*x[0])\n",
    "#         else:\n",
    "#             intensity_score.append(x[1])\n",
    "#     return intensity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_predicted_score = get_score_from_prob(train_predicted_prob)\n",
    "# test_predicted_score = get_score_from_prob(test_predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rmse = sqrt(mean_squared_error(score_trainY, train_predicted_score))\n",
    "# test_rmse = sqrt(mean_squared_error(score_testY, test_predicted_score))\n",
    "# print(\"(train rmse,test rmse)==\"+str((train_rmse,test_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# foo = [x for x in zip(score_testY,test_predicted_score)]\n",
    "# foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
