{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#multi channel CNN for sentiment analysis\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from pickle import dump,load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "def load_file_to_df(filename):\n",
    "    df = pd.read_csv(filename,delimiter='\\t',header=0)\n",
    "    df = df.drop(['Unnamed: 0', 'id'],axis=1)\n",
    "    df_text = df.iloc[:,:1]\n",
    "    df_score = df.iloc[:,1:]\n",
    "    return df_text,df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headlines_text,headlines_score = load_file_to_df(\"FiQA_train_ABSA_financial_headlines.tsv\")\n",
    "post_text,post_score = load_file_to_df(\"FiQA_train_ABSA_financial_posts.tsv\")\n",
    "text = pd.concat([headlines_text,post_text])\n",
    "score = pd.concat([headlines_score,post_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", sentence)\n",
    "    #removing stock names to see if it helps\n",
    "    sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract sentences out of df and cleaning it\n",
    "sentences = [clean_sentence(x) for x in text['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting real number scores to lables\n",
    "#0-->-ve sentiment 1-->+ve sentiment\n",
    "labels_df = (score>=0).astype(int)\n",
    "labels = [int(x) for x in labels_df['sentiment score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffling dataset\n",
    "numbers = [i for i in range(len(sentences))]\n",
    "shuffle(numbers)\n",
    "# numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3789\n",
      "3789\n"
     ]
    }
   ],
   "source": [
    "temp_text = sentences\n",
    "temp_lables = labels\n",
    "for i in numbers:\n",
    "    sentences[i] = temp_text[i]\n",
    "    labels[i]=temp_lables[i]\n",
    "print(len(sentences))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doing train and test split\n",
    "test_train_split_ratio =0.9\n",
    "trainX,testX = sentences[:int(test_train_split_ratio*len(sentences))],sentences[int(test_train_split_ratio*len(sentences)):]\n",
    "trainY,testY = labels[:int(test_train_split_ratio*len(labels))],labels[int(test_train_split_ratio*len(labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3410 3410\n",
      "379 379\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX),len(trainY))\n",
    "print(len(testX),len(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "# def max_length(lines):\n",
    "#     return max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testLines = [' '.join(x) for x in testX]\n",
    "trainLines = [' '.join(x) for x in trainX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainY = np.array(trainY)\n",
    "# testY = np.array(testY)\n",
    "# type(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 21\n",
      "Vocabulary size: 6019\n",
      "(3410, 21)\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "lenths = [len(s.split()) for s in trainLines]\n",
    "# length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 21, 100)      601900      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 21, 100)      601900      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 21, 100)      601900      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 18, 32)       12832       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 16, 32)       19232       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 14, 32)       25632       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 18, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 14, 32)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 9, 32)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 8, 32)        0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 7, 32)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 288)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 224)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           7690        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,871,097\n",
      "Trainable params: 1,871,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.6205 - acc: 0.6455\n",
      "Epoch 2/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.3118 - acc: 0.8745\n",
      "Epoch 3/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.1182 - acc: 0.9563\n",
      "Epoch 4/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0827 - acc: 0.9718\n",
      "Epoch 5/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0618 - acc: 0.9760\n",
      "Epoch 6/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0549 - acc: 0.9742\n",
      "Epoch 7/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0530 - acc: 0.9745\n",
      "Epoch 8/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0467 - acc: 0.9762\n",
      "Epoch 9/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0410 - acc: 0.9798\n",
      "Epoch 10/10\n",
      "3410/3410 [==============================] - 5s 2ms/step - loss: 0.0378 - acc: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e86576ef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], trainY, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3410, 21) (379, 21)\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "# encode data\n",
    "# trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.240469\n",
      "Test Accuracy: 81.266491\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate([trainX,trainX,trainX], trainY, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([testX,testX,testX], testY, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9433782696723938,\n",
       " 0.24506355822086334,\n",
       " 0.6104127168655396,\n",
       " 0.02297600917518139,\n",
       " 0.0012095182901248336,\n",
       " 0.01975431479513645,\n",
       " 0.9808628559112549,\n",
       " 0.9992669224739075,\n",
       " 0.485059916973114,\n",
       " 0.9460567831993103,\n",
       " 0.9908665418624878,\n",
       " 0.9998989105224609,\n",
       " 0.022076673805713654,\n",
       " 0.7359293699264526,\n",
       " 0.4027000665664673,\n",
       " 0.22554537653923035,\n",
       " 0.9346250295639038,\n",
       " 0.9972822666168213,\n",
       " 0.9975118637084961,\n",
       " 0.9830220937728882,\n",
       " 0.9723074436187744,\n",
       " 0.33366912603378296,\n",
       " 0.9229784607887268,\n",
       " 0.6720852255821228,\n",
       " 0.38315349817276,\n",
       " 0.9686875939369202,\n",
       " 0.26232418417930603,\n",
       " 0.9990881681442261,\n",
       " 0.9952555298805237,\n",
       " 0.17776653170585632,\n",
       " 0.7492717504501343,\n",
       " 0.06683600693941116,\n",
       " 0.9651023745536804,\n",
       " 0.9158439040184021,\n",
       " 0.0988103449344635,\n",
       " 0.8240588903427124,\n",
       " 0.6700507998466492,\n",
       " 0.0017795312451198697,\n",
       " 0.994307816028595,\n",
       " 0.14464259147644043,\n",
       " 0.7840084433555603,\n",
       " 0.8080596923828125,\n",
       " 0.12367767095565796,\n",
       " 0.12754641473293304,\n",
       " 0.9718340039253235,\n",
       " 0.15107415616512299,\n",
       " 0.9980852603912354,\n",
       " 0.9513480067253113,\n",
       " 0.12254243344068527,\n",
       " 0.0022604719270020723,\n",
       " 0.012751969508826733,\n",
       " 0.47896119952201843,\n",
       " 0.008351556025445461,\n",
       " 0.6157849431037903,\n",
       " 0.9575502872467041,\n",
       " 0.9056130051612854,\n",
       " 0.9971430897712708,\n",
       " 0.9999327659606934,\n",
       " 0.9990025162696838,\n",
       " 0.996040940284729,\n",
       " 0.9231955409049988,\n",
       " 0.5370994210243225,\n",
       " 0.9980073571205139,\n",
       " 0.4023498296737671,\n",
       " 0.9965118765830994,\n",
       " 0.0006818348774686456,\n",
       " 0.19533991813659668,\n",
       " 0.9789425730705261,\n",
       " 0.0039767115376889706,\n",
       " 0.0014422868844121695,\n",
       " 0.003282798919826746,\n",
       " 0.0022604719270020723,\n",
       " 0.9986667633056641,\n",
       " 0.9983687996864319,\n",
       " 0.9458725452423096,\n",
       " 0.9997919201850891,\n",
       " 0.9993423819541931,\n",
       " 0.0014422868844121695,\n",
       " 0.9972822666168213,\n",
       " 0.8808791041374207,\n",
       " 0.9778512120246887,\n",
       " 0.07246991991996765,\n",
       " 0.9468439817428589,\n",
       " 0.6074385643005371,\n",
       " 0.9967530369758606,\n",
       " 0.9966739416122437,\n",
       " 0.0003777027304749936,\n",
       " 0.006941615603864193,\n",
       " 0.945759117603302,\n",
       " 0.9397713541984558,\n",
       " 0.46624499559402466,\n",
       " 0.9904971122741699,\n",
       " 0.9783682823181152,\n",
       " 0.00420446926727891,\n",
       " 0.999839186668396,\n",
       " 0.16533724963665009,\n",
       " 0.006177224218845367,\n",
       " 0.9939935803413391,\n",
       " 0.9939935803413391,\n",
       " 0.9992823004722595,\n",
       " 0.9993212223052979,\n",
       " 0.9993212223052979,\n",
       " 0.9989944100379944,\n",
       " 0.9979650974273682,\n",
       " 0.0007161535322666168,\n",
       " 0.0014422868844121695,\n",
       " 0.9781700968742371,\n",
       " 0.32834291458129883,\n",
       " 0.9695085883140564,\n",
       " 0.3652872145175934,\n",
       " 0.22554537653923035,\n",
       " 0.0014422868844121695,\n",
       " 0.9999229907989502,\n",
       " 0.0020324287470430136,\n",
       " 0.9989431500434875,\n",
       " 0.9864718914031982,\n",
       " 0.999846339225769,\n",
       " 0.999846339225769,\n",
       " 0.9992321729660034,\n",
       " 0.7302172780036926,\n",
       " 0.9993212223052979,\n",
       " 0.9830220937728882,\n",
       " 0.0029678503051400185,\n",
       " 0.9999327659606934,\n",
       " 0.9988172650337219,\n",
       " 0.9988172650337219,\n",
       " 0.998329222202301,\n",
       " 0.0026434287428855896,\n",
       " 0.015107431448996067,\n",
       " 0.9995563626289368,\n",
       " 0.0033605715725570917,\n",
       " 0.9191233515739441,\n",
       " 0.9759601950645447,\n",
       " 0.9295737147331238,\n",
       " 0.720872163772583,\n",
       " 0.950850248336792,\n",
       " 0.9983687996864319,\n",
       " 0.9941608309745789,\n",
       " 0.9881636500358582,\n",
       " 0.8916724920272827,\n",
       " 0.9971582889556885,\n",
       " 0.6023437976837158,\n",
       " 0.9989781379699707,\n",
       " 0.5763196349143982,\n",
       " 0.999556839466095,\n",
       " 0.9629865884780884,\n",
       " 0.0003777027304749936,\n",
       " 0.04342953860759735,\n",
       " 0.9970607161521912,\n",
       " 0.0011736919404938817,\n",
       " 0.0016311310464516282,\n",
       " 0.9997689127922058,\n",
       " 0.6253745555877686,\n",
       " 0.9976696372032166,\n",
       " 0.0011736919404938817,\n",
       " 0.8174208402633667,\n",
       " 0.9974145889282227,\n",
       " 0.9972822666168213,\n",
       " 0.999658465385437,\n",
       " 0.9785547852516174,\n",
       " 0.8637980222702026,\n",
       " 0.001568122417666018,\n",
       " 0.9428960084915161,\n",
       " 0.09428567439317703,\n",
       " 0.8637980222702026,\n",
       " 0.9373335838317871,\n",
       " 0.989772379398346,\n",
       " 0.962047278881073,\n",
       " 0.9989431500434875,\n",
       " 0.0013668772298842669,\n",
       " 0.9089619517326355,\n",
       " 0.9993212223052979,\n",
       " 0.0003777027304749936,\n",
       " 0.9654041528701782,\n",
       " 0.0025481791235506535,\n",
       " 0.9949691891670227,\n",
       " 0.9993212223052979,\n",
       " 0.9993212223052979,\n",
       " 0.9993423819541931,\n",
       " 0.9993196725845337,\n",
       " 0.9977808594703674,\n",
       " 0.5458810329437256,\n",
       " 0.9974145889282227,\n",
       " 0.0022604719270020723,\n",
       " 0.2488359659910202,\n",
       " 0.00420446926727891,\n",
       " 0.9949691891670227,\n",
       " 0.9997785687446594,\n",
       " 0.9135234355926514,\n",
       " 0.999846339225769,\n",
       " 0.9830220937728882,\n",
       " 0.999846339225769,\n",
       " 0.9976770281791687,\n",
       " 0.7931288480758667,\n",
       " 0.9976857900619507,\n",
       " 0.00420446926727891,\n",
       " 0.0037062137853354216,\n",
       " 0.0003777027304749936,\n",
       " 0.0014422868844121695,\n",
       " 0.9997919201850891,\n",
       " 0.9972822666168213,\n",
       " 0.0007161535322666168,\n",
       " 0.9988635778427124,\n",
       " 0.015107431448996067,\n",
       " 0.9856206178665161,\n",
       " 0.0014422868844121695,\n",
       " 0.9994021654129028,\n",
       " 0.4968399405479431,\n",
       " 0.9871445298194885,\n",
       " 0.9994021654129028,\n",
       " 0.003881092183291912,\n",
       " 0.0037062137853354216,\n",
       " 0.9949769377708435,\n",
       " 0.9997919201850891,\n",
       " 0.9995094537734985,\n",
       " 0.015107431448996067,\n",
       " 0.0020982706919312477,\n",
       " 0.0033605715725570917,\n",
       " 0.959423840045929,\n",
       " 0.003881092183291912,\n",
       " 0.0037062137853354216,\n",
       " 0.0033605715725570917,\n",
       " 0.0006818348774686456,\n",
       " 0.9949691891670227,\n",
       " 0.998198926448822,\n",
       " 0.9964871406555176,\n",
       " 0.9960330128669739,\n",
       " 0.0003777027304749936,\n",
       " 0.9988172650337219,\n",
       " 0.0003777027304749936,\n",
       " 0.0013668772298842669,\n",
       " 0.999796450138092,\n",
       " 0.9992669224739075,\n",
       " 0.5838862061500549,\n",
       " 0.9930503368377686,\n",
       " 0.015859242528676987,\n",
       " 0.9964871406555176,\n",
       " 0.9925576448440552,\n",
       " 0.9871984124183655,\n",
       " 0.9979074001312256,\n",
       " 0.9998989105224609,\n",
       " 0.9701017737388611,\n",
       " 0.9976857900619507,\n",
       " 0.9757108688354492,\n",
       " 0.9830220937728882,\n",
       " 0.9832621812820435,\n",
       " 0.9977237582206726,\n",
       " 0.8381265997886658,\n",
       " 0.8474640846252441,\n",
       " 0.7679588198661804,\n",
       " 0.2811686396598816,\n",
       " 0.47545161843299866,\n",
       " 0.20935119688510895,\n",
       " 0.5370994210243225,\n",
       " 0.9908961653709412,\n",
       " 0.0014422868844121695,\n",
       " 0.857221782207489,\n",
       " 0.006433376111090183,\n",
       " 0.6806502938270569,\n",
       " 0.9989315867424011,\n",
       " 0.9988172650337219,\n",
       " 0.9995563626289368,\n",
       " 0.0014422868844121695,\n",
       " 0.9988172650337219,\n",
       " 0.9997919201850891,\n",
       " 0.9985232949256897,\n",
       " 0.9778512120246887,\n",
       " 0.6074385643005371,\n",
       " 0.9830220937728882,\n",
       " 0.9904971122741699,\n",
       " 0.9977237582206726,\n",
       " 0.0013668772298842669,\n",
       " 0.9949769377708435,\n",
       " 0.0014422868844121695,\n",
       " 0.2381541132926941,\n",
       " 0.7855783104896545,\n",
       " 0.9990556836128235,\n",
       " 0.20026268064975739,\n",
       " 0.9536879658699036,\n",
       " 0.8174208402633667,\n",
       " 0.9980852603912354,\n",
       " 0.9994021654129028,\n",
       " 0.9995594620704651,\n",
       " 0.9976696372032166,\n",
       " 0.045799367129802704,\n",
       " 0.9860625863075256,\n",
       " 0.27468523383140564,\n",
       " 0.9693952202796936,\n",
       " 0.39792487025260925,\n",
       " 0.3463331162929535,\n",
       " 0.29570916295051575,\n",
       " 0.9860625863075256,\n",
       " 0.2663613259792328,\n",
       " 0.9891198873519897,\n",
       " 0.041535455733537674,\n",
       " 0.9571664929389954,\n",
       " 0.9988172650337219,\n",
       " 0.9685609936714172,\n",
       " 0.9163896441459656,\n",
       " 0.9711036682128906,\n",
       " 0.8627547025680542,\n",
       " 0.2885008156299591,\n",
       " 0.5137284398078918,\n",
       " 0.0070496052503585815,\n",
       " 0.9222360253334045,\n",
       " 0.9992321729660034,\n",
       " 0.47968706488609314,\n",
       " 0.5383805632591248,\n",
       " 0.9992669224739075,\n",
       " 0.4636898636817932,\n",
       " 0.9949964284896851,\n",
       " 0.9908928871154785,\n",
       " 0.7886508107185364,\n",
       " 0.9998989105224609,\n",
       " 0.44188395142555237,\n",
       " 0.004656639415770769,\n",
       " 0.003881092183291912,\n",
       " 0.10976013541221619,\n",
       " 0.9948221445083618,\n",
       " 0.003469191724434495,\n",
       " 0.9830220937728882,\n",
       " 0.9908502101898193,\n",
       " 0.9974145889282227,\n",
       " 0.972173810005188,\n",
       " 0.9977237582206726,\n",
       " 0.9990881681442261,\n",
       " 0.9620391130447388,\n",
       " 0.995267391204834,\n",
       " 0.9992139339447021,\n",
       " 0.9971359968185425,\n",
       " 0.955996036529541,\n",
       " 0.9624179601669312,\n",
       " 0.005042020697146654,\n",
       " 0.9225847125053406,\n",
       " 0.008162936195731163,\n",
       " 0.7679588198661804,\n",
       " 0.006591263692826033,\n",
       " 0.9184731245040894,\n",
       " 0.006213750224560499,\n",
       " 0.09137877076864243,\n",
       " 0.9659672975540161,\n",
       " 0.0017795312451198697,\n",
       " 0.9856771230697632,\n",
       " 0.6603278517723083,\n",
       " 0.2811686396598816,\n",
       " 0.9995695948600769,\n",
       " 0.7081491947174072,\n",
       " 0.4982791841030121,\n",
       " 0.4271848201751709,\n",
       " 0.0037062137853354216,\n",
       " 0.1649966537952423,\n",
       " 0.9931380152702332,\n",
       " 0.9980852603912354,\n",
       " 0.9513480067253113,\n",
       " 0.19964616000652313,\n",
       " 0.992964506149292,\n",
       " 0.9526655673980713,\n",
       " 0.5011302828788757,\n",
       " 0.9971430897712708,\n",
       " 0.0033605715725570917,\n",
       " 0.9990025162696838,\n",
       " 0.9926716089248657,\n",
       " 0.5370994210243225,\n",
       " 0.6114104986190796,\n",
       " 0.9841594696044922,\n",
       " 0.9957460761070251,\n",
       " 0.6492947936058044,\n",
       " 0.9995187520980835,\n",
       " 0.5991957187652588,\n",
       " 0.9908502101898193,\n",
       " 0.999836802482605,\n",
       " 0.5396757125854492,\n",
       " 0.9946850538253784,\n",
       " 0.9995563626289368,\n",
       " 0.999846339225769,\n",
       " 0.999846339225769,\n",
       " 0.9999327659606934,\n",
       " 0.987776517868042,\n",
       " 0.22554537653923035]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_prob = [float(x) for x in model.predict([testX,testX,testX])]\n",
    "predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lables = []\n",
    "for x in predicted_prob:\n",
    "    if x>0.5:\n",
    "        predicted_lables.append(1)\n",
    "    else:\n",
    "        predicted_lables.append(0)\n",
    "predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
