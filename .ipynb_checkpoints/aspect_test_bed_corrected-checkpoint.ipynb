{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits\n",
    "import re\n",
    "from keras.utils import to_categorical\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "# from keras.utils.np_utils import probas_to_classes\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from random import shuffle\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "import word2vecReader as godin_embedding\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_json(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_dict = load_data_from_json('dataset/master_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data_from_dict(master_dict):\n",
    "    sentence = []\n",
    "    aspect = []\n",
    "    for key in master_dict.keys():\n",
    "        for info in master_dict[key]['info']:\n",
    "            sentence.append(eval(info['snippets'])[0])\n",
    "            aspect.append(eval(info['aspects'])[0])\n",
    "    return sentence,aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence,aspect = extract_data_from_dict(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1171, 1171)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence),len(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_level1= []\n",
    "aspect_level2=[]\n",
    "for asp in aspect:\n",
    "    try:\n",
    "        aspect_level1.append(asp.split('/')[0])\n",
    "        aspect_level2.append(asp.split('/')[1])\n",
    "    except:\n",
    "        print(asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_tsv_headline(filename):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    df.columns = [\"id\", \"sentence\", \"snippet\", \"score\",\"aspect\"]\n",
    "    snippet = df['snippet'].tolist()\n",
    "    aspect = df['aspect'].tolist()\n",
    "    \n",
    "    aspect_l2 = []\n",
    "    for asp in aspect:\n",
    "        try:\n",
    "            aspect_l2.append(asp.split('/')[1])\n",
    "        except:\n",
    "            print(asp)\n",
    "    \n",
    "    return snippet,aspect_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_headline_snippet,test_headline_aspect=load_data_from_tsv_headline('dataset/test_headlines_samples - Sheet1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_tsv_post(filename):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    df.columns = [\"id\", \"sentence\", \"snippet\", \"score\",'af','as1','as2','as3','as4']\n",
    "    snippet = df['snippet'].tolist()\n",
    "    aspect = df['as2'].tolist()\n",
    "    return snippet,aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_post_snippet,test_post_aspect = load_data_from_tsv_post('dataset/test_set_post - Sheet1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation.replace('$',''))\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,'')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1171, 1171, 99, 99, 93, 93)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence),len(aspect_level2),len(test_post_snippet),len(test_post_aspect),len(test_headline_snippet),len(test_headline_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataX = [clean_sentence(x) for x in sentence]\n",
    "test_headlineX = [clean_sentence(x) for x in test_headline_snippet]\n",
    "test_postX = [clean_sentence(x) for x in test_post_snippet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length  = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_of_classes = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataY = aspect_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_lables(trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,no_of_classes),le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading Google Word2Vec\n",
    "def load_google_word2vec(file_name):\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,free_em_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if em == 'free':\n",
    "        model.add(Embedding(vocab_size, free_em_dim))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag))\n",
    "    model.add(Bidirectional(LSTM(lstm_out, recurrent_dropout=dropout)))\n",
    "    for i in range(n_hidden_layer):\n",
    "        model.add(Dense(int((2*lstm_out+no_of_classes)/2),activation='relu'))\n",
    "    model.add(Dense(no_of_classes,activation='softmax'))\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model= load_google_word2vec('/home/hitkul/Desktop/ps3_iiit/ensembl_work/ensemble_experiments/word_embeddings/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(dataX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "trainX = encode_text(tokenizer, dataX, max_length)\n",
    "test_headlineX = encode_text(tokenizer, test_headlineX, max_length)\n",
    "test_postX = encode_text(tokenizer, test_postX, max_length)\n",
    "trainY,lable_encoding = convert_lables(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix_word2vec = get_word2vec_embedding_matrix(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 11, 300)           633900    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 800)               2243200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 413)               330813    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                11178     \n",
      "=================================================================\n",
      "Total params: 3,219,091\n",
      "Trainable params: 2,585,191\n",
      "Non-trainable params: 633,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = define_model(learning_rate=0.0006296220146941253,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=400,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_word2vec',\n",
    "                     em_trainable_flag=False,\n",
    "                     free_em_dim=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1171/1171 [==============================] - 19s 16ms/step - loss: 2.0695 - acc: 0.4705\n",
      "Epoch 2/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 1.2265 - acc: 0.6610\n",
      "Epoch 3/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.9051 - acc: 0.7464\n",
      "Epoch 4/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.6735 - acc: 0.7976\n",
      "Epoch 5/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.5600 - acc: 0.8207\n",
      "Epoch 6/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.4353 - acc: 0.8728\n",
      "Epoch 7/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.3341 - acc: 0.8907\n",
      "Epoch 8/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.2629 - acc: 0.9172\n",
      "Epoch 9/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.2467 - acc: 0.9172\n",
      "Epoch 10/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.2027 - acc: 0.9360\n",
      "Epoch 11/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.1526 - acc: 0.9547\n",
      "Epoch 12/100\n",
      "1171/1171 [==============================] - 19s 17ms/step - loss: 0.1588 - acc: 0.9505\n",
      "Epoch 13/100\n",
      "1171/1171 [==============================] - 18s 16ms/step - loss: 0.1656 - acc: 0.9505\n",
      "Epoch 14/100\n",
      "1171/1171 [==============================] - 20s 17ms/step - loss: 0.1146 - acc: 0.9650 0s - loss: 0.1168 - acc:\n",
      "Epoch 15/100\n",
      "1171/1171 [==============================] - 17s 14ms/step - loss: 0.1024 - acc: 0.9650\n",
      "Epoch 16/100\n",
      "1171/1171 [==============================] - 17s 14ms/step - loss: 0.0855 - acc: 0.9735\n",
      "Epoch 17/100\n",
      "1171/1171 [==============================] - 25s 21ms/step - loss: 0.0972 - acc: 0.9667\n",
      "Epoch 18/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0754 - acc: 0.9727\n",
      "Epoch 19/100\n",
      "1171/1171 [==============================] - 23s 19ms/step - loss: 0.1066 - acc: 0.9658\n",
      "Epoch 20/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.1230 - acc: 0.9599\n",
      "Epoch 21/100\n",
      "1171/1171 [==============================] - 22s 19ms/step - loss: 0.0479 - acc: 0.9846\n",
      "Epoch 22/100\n",
      "1171/1171 [==============================] - 24s 21ms/step - loss: 0.0485 - acc: 0.9863\n",
      "Epoch 23/100\n",
      "1171/1171 [==============================] - 23s 19ms/step - loss: 0.0538 - acc: 0.9863\n",
      "Epoch 24/100\n",
      "1171/1171 [==============================] - 23s 19ms/step - loss: 0.0682 - acc: 0.9778\n",
      "Epoch 25/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0562 - acc: 0.9795\n",
      "Epoch 26/100\n",
      "1171/1171 [==============================] - 20s 17ms/step - loss: 0.0958 - acc: 0.9684\n",
      "Epoch 27/100\n",
      "1171/1171 [==============================] - 22s 19ms/step - loss: 0.0620 - acc: 0.9804\n",
      "Epoch 28/100\n",
      "1171/1171 [==============================] - 22s 19ms/step - loss: 0.0444 - acc: 0.9821\n",
      "Epoch 29/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0303 - acc: 0.9880\n",
      "Epoch 30/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0254 - acc: 0.9915\n",
      "Epoch 31/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0229 - acc: 0.9915\n",
      "Epoch 32/100\n",
      "1171/1171 [==============================] - 30s 26ms/step - loss: 0.0608 - acc: 0.9829\n",
      "Epoch 33/100\n",
      "1171/1171 [==============================] - 26s 22ms/step - loss: 0.0395 - acc: 0.9872\n",
      "Epoch 34/100\n",
      "1171/1171 [==============================] - 24s 21ms/step - loss: 0.0388 - acc: 0.9889\n",
      "Epoch 35/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0577 - acc: 0.9846\n",
      "Epoch 36/100\n",
      "1171/1171 [==============================] - 25s 22ms/step - loss: 0.0351 - acc: 0.9855\n",
      "Epoch 37/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0242 - acc: 0.9898\n",
      "Epoch 38/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0503 - acc: 0.9872\n",
      "Epoch 39/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0911 - acc: 0.9684\n",
      "Epoch 40/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0735 - acc: 0.9795\n",
      "Epoch 41/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0298 - acc: 0.9898\n",
      "Epoch 42/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0276 - acc: 0.9906\n",
      "Epoch 43/100\n",
      "1171/1171 [==============================] - 24s 21ms/step - loss: 0.0304 - acc: 0.9889\n",
      "Epoch 44/100\n",
      "1171/1171 [==============================] - 24s 21ms/step - loss: 0.0339 - acc: 0.9872\n",
      "Epoch 45/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0766 - acc: 0.9761\n",
      "Epoch 46/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0689 - acc: 0.9812\n",
      "Epoch 47/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0280 - acc: 0.9880\n",
      "Epoch 48/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0209 - acc: 0.9915\n",
      "Epoch 49/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0202 - acc: 0.9906\n",
      "Epoch 50/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0241 - acc: 0.9906\n",
      "Epoch 51/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0138 - acc: 0.9932\n",
      "Epoch 52/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0434 - acc: 0.9863\n",
      "Epoch 53/100\n",
      "1171/1171 [==============================] - 23s 19ms/step - loss: 0.0320 - acc: 0.9872\n",
      "Epoch 54/100\n",
      "1171/1171 [==============================] - 22s 19ms/step - loss: 0.0252 - acc: 0.9898\n",
      "Epoch 55/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0174 - acc: 0.9906\n",
      "Epoch 56/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0160 - acc: 0.9906\n",
      "Epoch 57/100\n",
      "1171/1171 [==============================] - 23s 19ms/step - loss: 0.0153 - acc: 0.9940\n",
      "Epoch 58/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0145 - acc: 0.9932\n",
      "Epoch 59/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 60/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0167 - acc: 0.9923\n",
      "Epoch 61/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0194 - acc: 0.9923\n",
      "Epoch 62/100\n",
      "1171/1171 [==============================] - 23s 20ms/step - loss: 0.0797 - acc: 0.9735\n",
      "Epoch 63/100\n",
      "1171/1171 [==============================] - 24s 20ms/step - loss: 0.0485 - acc: 0.9795\n",
      "Epoch 64/100\n",
      "1171/1171 [==============================] - 19s 16ms/step - loss: 0.0300 - acc: 0.9889\n",
      "Epoch 65/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0545 - acc: 0.9829\n",
      "Epoch 66/100\n",
      "1171/1171 [==============================] - 21s 18ms/step - loss: 0.0428 - acc: 0.9855\n",
      "Epoch 67/100\n",
      "1171/1171 [==============================] - 28s 24ms/step - loss: 0.0206 - acc: 0.9906\n",
      "Epoch 68/100\n",
      "1171/1171 [==============================] - 21s 18ms/step - loss: 0.0430 - acc: 0.9863\n",
      "Epoch 69/100\n",
      "1171/1171 [==============================] - 20s 17ms/step - loss: 0.0259 - acc: 0.9880\n",
      "Epoch 70/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0223 - acc: 0.9906\n",
      "Epoch 71/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0184 - acc: 0.9915\n",
      "Epoch 72/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0215 - acc: 0.9889\n",
      "Epoch 73/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0142 - acc: 0.9923\n",
      "Epoch 74/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0139 - acc: 0.9923\n",
      "Epoch 75/100\n",
      "1171/1171 [==============================] - 18s 16ms/step - loss: 0.0120 - acc: 0.9940\n",
      "Epoch 76/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0132 - acc: 0.9923\n",
      "Epoch 77/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0260 - acc: 0.9906\n",
      "Epoch 78/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0173 - acc: 0.9923\n",
      "Epoch 79/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0195 - acc: 0.9915\n",
      "Epoch 80/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0388 - acc: 0.9829\n",
      "Epoch 81/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0265 - acc: 0.9923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.0361 - acc: 0.9889\n",
      "Epoch 83/100\n",
      "1171/1171 [==============================] - 18s 16ms/step - loss: 0.0288 - acc: 0.9872\n",
      "Epoch 84/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0172 - acc: 0.9923\n",
      "Epoch 85/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 86/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.0139 - acc: 0.9940\n",
      "Epoch 87/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0151 - acc: 0.9906\n",
      "Epoch 88/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0135 - acc: 0.9940\n",
      "Epoch 89/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0119 - acc: 0.9932\n",
      "Epoch 90/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0123 - acc: 0.9932\n",
      "Epoch 91/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0139 - acc: 0.9915\n",
      "Epoch 92/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0111 - acc: 0.9932\n",
      "Epoch 93/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0372 - acc: 0.9872\n",
      "Epoch 94/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0977 - acc: 0.9710\n",
      "Epoch 95/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0515 - acc: 0.9821\n",
      "Epoch 96/100\n",
      "1171/1171 [==============================] - 17s 15ms/step - loss: 0.0307 - acc: 0.9889\n",
      "Epoch 97/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0131 - acc: 0.9940\n",
      "Epoch 98/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0162 - acc: 0.9923\n",
      "Epoch 99/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0220 - acc: 0.9889\n",
      "Epoch 100/100\n",
      "1171/1171 [==============================] - 18s 15ms/step - loss: 0.0124 - acc: 0.9932\n"
     ]
    }
   ],
   "source": [
    "history_object = model.fit(trainX, trainY, epochs=100,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [lable_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_headline = model.predict(test_headlineX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_class_headline = get_class_from_pred(pred_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14448682630500811"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_headline_aspect,pred_class_headline,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32704463994786576"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_headline_aspect,pred_class_headline,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_post = model.predict(test_postX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_class_post = get_class_from_pred(pred_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3501794477404233"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_post_aspect,pred_class_post,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/hitkul/anaconda3/envs/ps3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8980628818027192"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_post_aspect,pred_class_post,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
