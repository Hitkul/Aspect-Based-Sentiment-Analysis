{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from math import exp\n",
    "import matplotlib.pyplot as py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_json(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_data = load_data_from_json('dataset/master.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_post(s):\n",
    "    if len(re.findall(r'\\$([a-zA-Z_]+)',s))>0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seprating post and headlines\n",
    "post_sentence = []\n",
    "headlines_sentence = []\n",
    "for key in master_data.keys():\n",
    "    if is_post(master_data[key]['sentence']):\n",
    "        post_sentence.append(master_data[key]['sentence'])\n",
    "    else:\n",
    "        headlines_sentence.append(master_data[key]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 438)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_sentence),len(headlines_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #removing stock names to see if it helps\n",
    "#     sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation.replace('$',''))\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "#     remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sentence = [clean_sentence(s) for s in post_sentence]\n",
    "headlines_sentence = [clean_sentence(s) for s in headlines_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sentence(s):\n",
    "    sentences = nltk.sent_tokenize(s)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    return chunked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ners(s):\n",
    "    chunked_sentences = prepare_sentence(s)\n",
    "    entity_names = []\n",
    "    for tree in chunked_sentences:\n",
    "        entity_names.extend(extract_entity_names(tree))\n",
    "    entity_names = [x.split()[0] for x in list(set(entity_names))]\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index_of_targets(s,is_post_flag = True):\n",
    "    if is_post_flag:\n",
    "        targets = ['$'+x for x in re.findall(r'\\$([a-zA-Z_]+)',s)]\n",
    "        index = [i for i,j in enumerate(s.split()) if j in targets]\n",
    "        return index\n",
    "    else:\n",
    "        targets = get_ners(s)\n",
    "        index = [i for i,j in enumerate(s.split()) if j in targets]\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_dependency_tree(s):\n",
    "    nlp = spacy.load('en')\n",
    "    document = nlp(s)\n",
    "\n",
    "    # Load spacy's dependency tree into a networkx graph\n",
    "    edges = []\n",
    "    for token in document:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                          '{0}-{1}'.format(child.lower_,child.i)))\n",
    "    graph = nx.DiGraph(edges)\n",
    "    for node in graph.in_degree():\n",
    "        if node[1] == 0:\n",
    "            root = node[0]\n",
    "            break\n",
    "    nodes = graph.node()\n",
    "    depth = 0\n",
    "    for node in nodes:\n",
    "        temp = nx.shortest_path_length(graph, source=root,target=node)\n",
    "        if temp > depth:\n",
    "            depth = temp\n",
    "    return graph, depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distance_between_two_words(graph,node1,node1_index,node2,node2_index):\n",
    "    node1 = node1.lower().replace('$','')\n",
    "    node1 = node1+'-'+str(node1_index)\n",
    "    node2 = node2.lower().replace('$','')\n",
    "    node2 = node2+'-'+str(node2_index)\n",
    "    return nx.shortest_path_length(graph.to_undirected(), source=node1, target=node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_tokens_prob(s):\n",
    "    s_prob_vectors = []\n",
    "    tokens = s.split()\n",
    "    prob_target = np.zeros(len(tokens))\n",
    "    if is_post(s):\n",
    "        target_index = get_index_of_targets(s)\n",
    "    else:\n",
    "        target_index = get_index_of_targets(s,is_post_flag=False)\n",
    "    prob_each_target = 1.0/len(target_index)\n",
    "    graph,depth = get_sentence_dependency_tree(s.replace('$',''))\n",
    "    for i in target_index:\n",
    "        prob_target[i] = prob_each_target\n",
    "    for i in range(len(prob_target)):\n",
    "        if prob_target[i]!=0:\n",
    "            sentence_prob = np.zeros(len(tokens))\n",
    "            for j in range(len(sentence_prob)):\n",
    "                if j==i:\n",
    "                    sentence_prob[j]+=1+prob_target[i]\n",
    "                else:\n",
    "                    sentence_prob[j]+=prob_target[i]*exp(-((get_distance_between_two_words(graph,tokens[i],i,tokens[j],j)**2)/(2.0*depth)))\n",
    "            s_prob_vectors.append(sentence_prob)\n",
    "    return s_prob_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_relation_vector = get_sentence_tokens_prob('Royal Mail chairman Donald Brydon set to step down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalize_series(series):\n",
    "    mean_series = np.mean(series)\n",
    "    std_series = np.std(series)\n",
    "    series_normalized = [(x-mean_series)/std_series for x in series]\n",
    "    return [x+1 for x in series_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.6876491924228207,\n",
       "  1.3434325122150745,\n",
       "  1.0379195377574018,\n",
       "  0.516274935433043,\n",
       "  0.725861642252898,\n",
       "  0.516274935433043,\n",
       "  0.37874817868164057,\n",
       "  0.41509088712243847,\n",
       "  0.37874817868164057],\n",
       " [0.3588265987052004,\n",
       "  0.5802940998568424,\n",
       "  0.9100415508315126,\n",
       "  3.7099758322100933,\n",
       "  1.2328730702975956,\n",
       "  0.9100415508315126,\n",
       "  0.3588265987052004,\n",
       "  0.5802940998568424,\n",
       "  0.3588265987052004]]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_relation_vector = [renormalize_series(x) for x in sentence_relation_vector]\n",
    "sentence_relation_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall(r'\\$([a-zA-Z]+)','Slowly adding some $FIO here $googl but gotta be $12 careful. This will be one of biggest winners in 2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post_sentence = []\n",
    "# post_target = []\n",
    "# extracted_re = []\n",
    "# for key in posts_data.keys():\n",
    "#     post_sentence.append(posts_data[key]['sentence'])\n",
    "#     temp = []\n",
    "#     for x in posts_data[key]['info']:\n",
    "#         temp.append(x['target'])\n",
    "#     post_target.append(temp)\n",
    "#     extracted_re.append(list(set(re.findall(r'\\$([a-zA-Z_]+)',posts_data[key]['sentence']))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(post_sentence),len(post_target),len(extracted_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in enumerate(zip(target,extracted_re)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,a in enumerate(extracted_re):\n",
    "#     if len(a)>1:\n",
    "#         print(sentence[i])\n",
    "#         print(target[i],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headlines_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNP_tokens=[]\n",
    "# headlines=[]\n",
    "# targets_headline = []\n",
    "# for key in headlines_data.keys():\n",
    "#     text = nltk.word_tokenize(headlines_data[key]['sentence'])\n",
    "#     print(\"-------------------------------------------------------\")\n",
    "#     print(headlines_data[key]['sentence'])\n",
    "#     headlines.append(headlines_data[key]['sentence'])\n",
    "#     temp = []\n",
    "#     for x in headlines_data[key]['info']:\n",
    "#         temp.append(x['target'])\n",
    "#     print(temp)\n",
    "#     targets_headline.append(temp)\n",
    "#     pos_tags = nltk.pos_tag(text)\n",
    "#     for x in pos_tags:\n",
    "#         if x[1] == 'NNP':\n",
    "#             NNP_tokens.append(x[0])\n",
    "#     print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating word freq  \n",
    "# nnp_tokens_freq = nltk.probability.FreqDist(NNP_tokens)\n",
    "# nnp_tokens_freq.pprint(len(NNP_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#expermenting with NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# headlines = [headlines_data[key]['sentence'] for key in headlines_data.keys()]\n",
    "# targets = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ners_headlines = [get_ners(s) for s in headlines]\n",
    "# ners_headlines = [(nltk.pos_tag(n)) for n in ners_headlines]\n",
    "# ners_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in enumerate(zip(targets_headline,ners_headlines)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headlines[284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ners_post = [get_ners(s) for s in post_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in enumerate(zip(post_target,ners_post)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
