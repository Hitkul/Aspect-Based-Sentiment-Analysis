{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from math import exp\n",
    "import matplotlib.pyplot as py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_json(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_data = load_data_from_json('dataset/master.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_post(s):\n",
    "    if len(re.findall(r'\\$([a-zA-Z_]+)',s))>0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seprating post and headlines\n",
    "post_sentence = []\n",
    "headlines_sentence = []\n",
    "for key in master_data.keys():\n",
    "    if is_post(master_data[key]['sentence']):\n",
    "        post_sentence.append(master_data[key]['sentence'])\n",
    "    else:\n",
    "        headlines_sentence.append(master_data[key]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 436)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_sentence),len(headlines_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    #removing stock names to see if it helps\n",
    "#     sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation.replace('$',''))\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "#     remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_sentence = [clean_sentence(s) for s in post_sentence]\n",
    "headlines_sentence = [clean_sentence(s) for s in headlines_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sentence(s):\n",
    "    sentences = nltk.sent_tokenize(s)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    return chunked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ners(s):\n",
    "    chunked_sentences = prepare_sentence(s)\n",
    "    entity_names = []\n",
    "    for tree in chunked_sentences:\n",
    "        entity_names.extend(extract_entity_names(tree))\n",
    "    entity_names = [x.split()[0] for x in list(set(entity_names))]\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index_of_targets(s,is_post_flag = True):\n",
    "    if is_post_flag:\n",
    "        targets = ['$'+x for x in re.findall(r'\\$([a-zA-Z_]+)',s)]\n",
    "        if len(targets)==0:\n",
    "            print('no target post')\n",
    "        index = [i for i,j in enumerate(s.split()) if j in targets]\n",
    "        return index\n",
    "    else:\n",
    "        targets = get_ners(s)\n",
    "        if len(targets)==0:\n",
    "            print('no target headline')\n",
    "        index = [i for i,j in enumerate(s.split()) if j in targets]\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_dependency_tree(s):\n",
    "    nlp = spacy.load('en')\n",
    "    document = nlp(s)\n",
    "\n",
    "    # Load spacy's dependency tree into a networkx graph\n",
    "    edges = []\n",
    "    for token in document:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                          '{0}-{1}'.format(child.lower_,child.i)))\n",
    "    graph = nx.DiGraph(edges)\n",
    "    for node in graph.in_degree():\n",
    "        if node[1] == 0:\n",
    "            root = node[0]\n",
    "            break\n",
    "    nodes = graph.node()\n",
    "    depth = 0\n",
    "    for node in nodes:\n",
    "        temp = nx.shortest_path_length(graph, source=root,target=node)\n",
    "        if temp > depth:\n",
    "            depth = temp\n",
    "    return graph, depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distance_between_two_words(graph,node1,node1_index,node2,node2_index,depth):\n",
    "    node1 = node1.lower().replace('$','')\n",
    "    node1 = node1+'-'+str(node1_index)\n",
    "    node2 = node2.lower().replace('$','')\n",
    "    node2 = node2+'-'+str(node2_index)\n",
    "    try:\n",
    "        return nx.shortest_path_length(graph.to_undirected(), source=node1, target=node2)\n",
    "    except:\n",
    "        return 10*depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_tokens_prob(s):\n",
    "    s_prob_vectors = []\n",
    "    tokens = s.split()\n",
    "    prob_target = np.zeros(len(tokens))\n",
    "    if is_post(s):\n",
    "        target_index = get_index_of_targets(s)\n",
    "    else:\n",
    "        target_index = get_index_of_targets(s,is_post_flag=False)\n",
    "    prob_each_target = 1.0/len(target_index)\n",
    "    graph,depth = get_sentence_dependency_tree(s.replace('$',''))\n",
    "    for i in target_index:\n",
    "        prob_target[i] = prob_each_target\n",
    "    for i in range(len(prob_target)):\n",
    "        if prob_target[i]!=0:\n",
    "            sentence_prob = np.zeros(len(tokens))\n",
    "            for j in range(len(sentence_prob)):\n",
    "                if j==i:\n",
    "                    sentence_prob[j]+=1+prob_target[i]\n",
    "                else:\n",
    "                    sentence_prob[j]+=prob_target[i]*exp(-((get_distance_between_two_words(graph,tokens[i],i,tokens[j],j,depth)**2)/(2.0*depth)))\n",
    "            s_prob_vectors.append(sentence_prob)\n",
    "    return s_prob_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NetworkXNoPath",
     "evalue": "No path between hold-7 and keep-11.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNetworkXNoPath\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2b89a49ac929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpost_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mget_sentence_tokens_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-56c50eb89e1e>\u001b[0m in \u001b[0;36mget_sentence_tokens_prob\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_index_of_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_post_flag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprob_each_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_dependency_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprob_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_each_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-53351e9f23d6>\u001b[0m in \u001b[0;36mget_sentence_dependency_tree\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/generic.py\u001b[0m in \u001b[0;36mshortest_path_length\u001b[0;34m(G, source, target, weight)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# Find shortest source-target path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional_shortest_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36mbidirectional_shortest_path\u001b[0;34m(G, source, target)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# call helper to do the real work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bidirectional_pred_succ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ps3/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36m_bidirectional_pred_succ\u001b[0;34m(G, source, target)\u001b[0m\n\u001b[1;32m    292\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkXNoPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No path between %s and %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNetworkXNoPath\u001b[0m: No path between hold-7 and keep-11."
     ]
    }
   ],
   "source": [
    "for x in post_sentence:\n",
    "    get_sentence_tokens_prob(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.5       , 0.44124845, 0.30326533, 0.06766764, 0.16232623,\n",
       "        0.06766764, 0.0055545 , 0.02196847, 0.0055545 ]),\n",
       " array([0.06766764, 0.16232623, 0.30326533, 1.5       , 0.44124845,\n",
       "        0.30326533, 0.06766764, 0.16232623, 0.06766764])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_relation_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def renormalize_series(series):\n",
    "    mean_series = np.mean(series)\n",
    "    std_series = np.std(series)\n",
    "    series_normalized = [(x-mean_series)/std_series for x in series]\n",
    "    return [x+1 for x in series_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.6876491924228207,\n",
       "  1.3434325122150745,\n",
       "  1.0379195377574018,\n",
       "  0.516274935433043,\n",
       "  0.725861642252898,\n",
       "  0.516274935433043,\n",
       "  0.37874817868164057,\n",
       "  0.41509088712243847,\n",
       "  0.37874817868164057],\n",
       " [0.3588265987052004,\n",
       "  0.5802940998568424,\n",
       "  0.9100415508315126,\n",
       "  3.7099758322100933,\n",
       "  1.2328730702975956,\n",
       "  0.9100415508315126,\n",
       "  0.3588265987052004,\n",
       "  0.5802940998568424,\n",
       "  0.3588265987052004]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_relation_vector = [renormalize_series(x) for x in sentence_relation_vector]\n",
    "sentence_relation_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re.findall(r'\\$([a-zA-Z]+)','Slowly adding some $FIO here $googl but gotta be $12 careful. This will be one of biggest winners in 2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post_sentence = []\n",
    "# post_target = []\n",
    "# extracted_re = []\n",
    "# for key in posts_data.keys():\n",
    "#     post_sentence.append(posts_data[key]['sentence'])\n",
    "#     temp = []\n",
    "#     for x in posts_data[key]['info']:\n",
    "#         temp.append(x['target'])\n",
    "#     post_target.append(temp)\n",
    "#     extracted_re.append(list(set(re.findall(r'\\$([a-zA-Z_]+)',posts_data[key]['sentence']))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(post_sentence),len(post_target),len(extracted_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in enumerate(zip(target,extracted_re)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i,a in enumerate(extracted_re):\n",
    "#     if len(a)>1:\n",
    "#         print(sentence[i])\n",
    "#         print(target[i],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# headlines_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NNP_tokens=[]\n",
    "# headlines=[]\n",
    "# targets_headline = []\n",
    "# for key in headlines_data.keys():\n",
    "#     text = nltk.word_tokenize(headlines_data[key]['sentence'])\n",
    "#     print(\"-------------------------------------------------------\")\n",
    "#     print(headlines_data[key]['sentence'])\n",
    "#     headlines.append(headlines_data[key]['sentence'])\n",
    "#     temp = []\n",
    "#     for x in headlines_data[key]['info']:\n",
    "#         temp.append(x['target'])\n",
    "#     print(temp)\n",
    "#     targets_headline.append(temp)\n",
    "#     pos_tags = nltk.pos_tag(text)\n",
    "#     for x in pos_tags:\n",
    "#         if x[1] == 'NNP':\n",
    "#             NNP_tokens.append(x[0])\n",
    "#     print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating word freq  \n",
    "# nnp_tokens_freq = nltk.probability.FreqDist(NNP_tokens)\n",
    "# nnp_tokens_freq.pprint(len(NNP_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#expermenting with NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# headlines = [headlines_data[key]['sentence'] for key in headlines_data.keys()]\n",
    "# targets = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ners_headlines = [get_ners(s) for s in headlines]\n",
    "# ners_headlines = [(nltk.pos_tag(n)) for n in ners_headlines]\n",
    "# ners_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in enumerate(zip(targets_headline,ners_headlines)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# headlines[284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# posts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ners_post = [get_ners(s) for s in post_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in enumerate(zip(post_target,ners_post)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
